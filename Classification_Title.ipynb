{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate, BatchNormalization\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les Train et test pour les titres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (8000,), (2000,), (2000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/title_only\"\n",
    "feature = \"title\"\n",
    "label = 'label'\n",
    "x_train = pd.read_csv(f\"{data_dir}/train.csv\")[feature]\n",
    "y_train = pd.read_csv(f\"{data_dir}/train.csv\")[label]\n",
    "\n",
    "x_test = pd.read_csv(f\"{data_dir}/test.csv\")[feature]\n",
    "y_test = pd.read_csv(f\"{data_dir}/test.csv\")[label]\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manipulation de y:cat√©gorie\n",
    "def create_vocab(dt): \n",
    "  to_id = {'<PAD>': 0, '<UNK>':1}\n",
    "\n",
    "  for sent in dt:\n",
    "    for w in sent: \n",
    "      if w not in to_id.keys():\n",
    "        to_id[w] = len(to_id)\n",
    "\n",
    "  from_id = {v: k for k, v in to_id.items()}\n",
    "\n",
    "  vocab = len(to_id.keys())\n",
    "\n",
    "  return to_id, from_id, vocab\n",
    "\n",
    "def preprocess_Y(Y, cat_to_id): \n",
    "  res = []\n",
    "  for ex in Y: \n",
    "    if ex not in cat_to_id.keys():\n",
    "      res.append(cat_to_id['<UNK>'])\n",
    "    else:\n",
    "      res.append(cat_to_id[ex])\n",
    "  return np.array(res)\n",
    "\n",
    "cat_to_id, cat_from_id, cat_vocab = create_vocab([y_train])\n",
    "y_train_id = preprocess_Y(y_train, cat_to_id)\n",
    "y_test_id = preprocess_Y(y_test, cat_to_id)\n",
    "\n",
    "#Manipulation des titres \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq =  tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_seq = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen = 40, truncating='post')\n",
    "x_test_seq = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen = 40, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#une fonction pour visualiser accuracy et loss\n",
    "def plot_history(history, metric=\"acc\"):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.title(f'model {metric}')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'of': 1,\n",
       " 'and': 2,\n",
       " 'in': 3,\n",
       " 'for': 4,\n",
       " 'the': 5,\n",
       " 'a': 6,\n",
       " 'using': 7,\n",
       " 'to': 8,\n",
       " 'with': 9,\n",
       " 'neural': 10,\n",
       " 'based': 11,\n",
       " 'learning': 12,\n",
       " 'on': 13,\n",
       " 'analysis': 14,\n",
       " 'robotic': 15,\n",
       " 'network': 16,\n",
       " 'by': 17,\n",
       " 'networks': 18,\n",
       " 'an': 19,\n",
       " 'assisted': 20,\n",
       " 'artificial': 21,\n",
       " 'from': 22,\n",
       " 'robot': 23,\n",
       " 'prediction': 24,\n",
       " 'machine': 25,\n",
       " 'system': 26,\n",
       " 'cancer': 27,\n",
       " 'deep': 28,\n",
       " 'data': 29,\n",
       " 'laparoscopic': 30,\n",
       " 'classification': 31,\n",
       " 'surgery': 32,\n",
       " 'model': 33,\n",
       " 'approach': 34,\n",
       " 'detection': 35,\n",
       " 'protein': 36,\n",
       " 'study': 37,\n",
       " 'prostatectomy': 38,\n",
       " 'human': 39,\n",
       " 'fuzzy': 40,\n",
       " 'radical': 41,\n",
       " 'gene': 42,\n",
       " 'computer': 43,\n",
       " 'patients': 44,\n",
       " 'identification': 45,\n",
       " 'disease': 46,\n",
       " 'automated': 47,\n",
       " 'support': 48,\n",
       " 'method': 49,\n",
       " 'images': 50,\n",
       " 'application': 51,\n",
       " 'brain': 52,\n",
       " 'diagnosis': 53,\n",
       " 'image': 54,\n",
       " 'control': 55,\n",
       " 'novel': 56,\n",
       " 'predicting': 57,\n",
       " 'imaging': 58,\n",
       " 'models': 59,\n",
       " 'during': 60,\n",
       " 'clinical': 61,\n",
       " 'comparison': 62,\n",
       " 'new': 63,\n",
       " 'development': 64,\n",
       " 'expression': 65,\n",
       " 'evaluation': 66,\n",
       " 'surgical': 67,\n",
       " 'algorithm': 68,\n",
       " 'cell': 69,\n",
       " 'functional': 70,\n",
       " 'modeling': 71,\n",
       " 'systems': 72,\n",
       " 'intelligence': 73,\n",
       " 'vector': 74,\n",
       " 'recognition': 75,\n",
       " 'use': 76,\n",
       " 'automatic': 77,\n",
       " 'assessment': 78,\n",
       " 'time': 79,\n",
       " 'high': 80,\n",
       " 'drug': 81,\n",
       " 'as': 82,\n",
       " 'after': 83,\n",
       " 'single': 84,\n",
       " 'structure': 85,\n",
       " 'breast': 86,\n",
       " 'segmentation': 87,\n",
       " 'information': 88,\n",
       " 'methods': 89,\n",
       " 'multi': 90,\n",
       " 'review': 91,\n",
       " 'between': 92,\n",
       " 'prostate': 93,\n",
       " 'activity': 94,\n",
       " 'genes': 95,\n",
       " 'outcomes': 96,\n",
       " 'feature': 97,\n",
       " 'decision': 98,\n",
       " 'training': 99,\n",
       " 'function': 100,\n",
       " 'knowledge': 101,\n",
       " 'features': 102,\n",
       " 'design': 103,\n",
       " 'computational': 104,\n",
       " 'genome': 105,\n",
       " 'mri': 106,\n",
       " 'is': 107,\n",
       " 'risk': 108,\n",
       " 'lung': 109,\n",
       " 'medical': 110,\n",
       " 'convolutional': 111,\n",
       " 'performance': 112,\n",
       " 'multiple': 113,\n",
       " 'comparative': 114,\n",
       " 'selection': 115,\n",
       " 'robotics': 116,\n",
       " 'experience': 117,\n",
       " 'invasive': 118,\n",
       " 'treatment': 119,\n",
       " 'quantitative': 120,\n",
       " 'non': 121,\n",
       " 'transcriptome': 122,\n",
       " 'versus': 123,\n",
       " 'techniques': 124,\n",
       " 'optimization': 125,\n",
       " 'early': 126,\n",
       " 'technique': 127,\n",
       " 'ct': 128,\n",
       " 'cells': 129,\n",
       " 'processing': 130,\n",
       " 'aided': 131,\n",
       " 'at': 132,\n",
       " 'case': 133,\n",
       " 'genetic': 134,\n",
       " 'response': 135,\n",
       " 'partial': 136,\n",
       " 'patterns': 137,\n",
       " 'adaptive': 138,\n",
       " 'spectroscopy': 139,\n",
       " 'proteins': 140,\n",
       " 'management': 141,\n",
       " 'visual': 142,\n",
       " 'open': 143,\n",
       " 'specific': 144,\n",
       " 'two': 145,\n",
       " 'complex': 146,\n",
       " 'quality': 147,\n",
       " 'patient': 148,\n",
       " 'self': 149,\n",
       " 'molecular': 150,\n",
       " 'through': 151,\n",
       " 'associated': 152,\n",
       " 'ontology': 153,\n",
       " 'improved': 154,\n",
       " 'extraction': 155,\n",
       " 'approaches': 156,\n",
       " 'hybrid': 157,\n",
       " 'minimally': 158,\n",
       " 'potential': 159,\n",
       " 'pattern': 160,\n",
       " 'dynamic': 161,\n",
       " 'technology': 162,\n",
       " 'applications': 163,\n",
       " 'future': 164,\n",
       " 'care': 165,\n",
       " 'long': 166,\n",
       " 'role': 167,\n",
       " 'binding': 168,\n",
       " 'term': 169,\n",
       " 'machines': 170,\n",
       " 'large': 171,\n",
       " 'expert': 172,\n",
       " 'interaction': 173,\n",
       " 'its': 174,\n",
       " 'related': 175,\n",
       " 'via': 176,\n",
       " 'screening': 177,\n",
       " 'rna': 178,\n",
       " 'clustering': 179,\n",
       " 'effect': 180,\n",
       " 'health': 181,\n",
       " 'infrared': 182,\n",
       " 'current': 183,\n",
       " 'estimation': 184,\n",
       " 'nephrectomy': 185,\n",
       " 'or': 186,\n",
       " 'tool': 187,\n",
       " 'magnetic': 188,\n",
       " 'dimensional': 189,\n",
       " '1': 190,\n",
       " 'sequence': 191,\n",
       " 'motor': 192,\n",
       " 'eeg': 193,\n",
       " 'algorithms': 194,\n",
       " 'reveals': 195,\n",
       " 'biological': 196,\n",
       " 'cortex': 197,\n",
       " 'regression': 198,\n",
       " 'three': 199,\n",
       " 'their': 200,\n",
       " 'natural': 201,\n",
       " 'stress': 202,\n",
       " 'wide': 203,\n",
       " 'reconstruction': 204,\n",
       " 'characterization': 205,\n",
       " 'tissue': 206,\n",
       " 'different': 207,\n",
       " 'diagnostic': 208,\n",
       " 'interactions': 209,\n",
       " 'research': 210,\n",
       " 'dna': 211,\n",
       " 'impact': 212,\n",
       " 'structural': 213,\n",
       " 'results': 214,\n",
       " 'therapy': 215,\n",
       " 'determination': 216,\n",
       " 'medicine': 217,\n",
       " 'state': 218,\n",
       " 'near': 219,\n",
       " 'initial': 220,\n",
       " 'predict': 221,\n",
       " 'tomography': 222,\n",
       " 'real': 223,\n",
       " 'cardiac': 224,\n",
       " 'resonance': 225,\n",
       " 'induced': 226,\n",
       " 'identifying': 227,\n",
       " 'mr': 228,\n",
       " 'robots': 229,\n",
       " 'changes': 230,\n",
       " 'evolution': 231,\n",
       " 'sites': 232,\n",
       " 'dynamics': 233,\n",
       " 'into': 234,\n",
       " 'logic': 235,\n",
       " 'can': 236,\n",
       " 'systematic': 237,\n",
       " 'de': 238,\n",
       " 'simulation': 239,\n",
       " 'scale': 240,\n",
       " 'renal': 241,\n",
       " 'database': 242,\n",
       " 'guided': 243,\n",
       " 'monitoring': 244,\n",
       " 'small': 245,\n",
       " 'sequencing': 246,\n",
       " 'carcinoma': 247,\n",
       " 'effects': 248,\n",
       " 'supervised': 249,\n",
       " 'cognitive': 250,\n",
       " 'first': 251,\n",
       " 'making': 252,\n",
       " 'liver': 253,\n",
       " 'recurrent': 254,\n",
       " 'spatial': 255,\n",
       " 'blood': 256,\n",
       " 'mining': 257,\n",
       " 'enhanced': 258,\n",
       " 'driven': 259,\n",
       " 'framework': 260,\n",
       " 'are': 261,\n",
       " '3d': 262,\n",
       " 'discovery': 263,\n",
       " 'statistical': 264,\n",
       " '2': 265,\n",
       " 'memory': 266,\n",
       " 'trial': 267,\n",
       " 'representation': 268,\n",
       " 'basis': 269,\n",
       " 'accuracy': 270,\n",
       " 'integration': 271,\n",
       " 'ultrasound': 272,\n",
       " 'intelligent': 273,\n",
       " 'level': 274,\n",
       " 'profiles': 275,\n",
       " 'validation': 276,\n",
       " 'body': 277,\n",
       " 'improving': 278,\n",
       " 'rapid': 279,\n",
       " 'mass': 280,\n",
       " 'acute': 281,\n",
       " 'low': 282,\n",
       " 'local': 283,\n",
       " 'phase': 284,\n",
       " 'active': 285,\n",
       " 'tumor': 286,\n",
       " 'nonlinear': 287,\n",
       " 'optical': 288,\n",
       " 'endoscopic': 289,\n",
       " 'profiling': 290,\n",
       " 'water': 291,\n",
       " 'field': 292,\n",
       " 'report': 293,\n",
       " 'cystectomy': 294,\n",
       " 'combined': 295,\n",
       " 'mapping': 296,\n",
       " 'pathways': 297,\n",
       " 'controlled': 298,\n",
       " 'coronary': 299,\n",
       " 'total': 300,\n",
       " 'sequences': 301,\n",
       " 'factors': 302,\n",
       " 'localization': 303,\n",
       " 'temporal': 304,\n",
       " 'optimal': 305,\n",
       " 'novo': 306,\n",
       " 'short': 307,\n",
       " 'ensemble': 308,\n",
       " 'accurate': 309,\n",
       " 'target': 310,\n",
       " 'outcome': 311,\n",
       " 'integrated': 312,\n",
       " 'stroke': 313,\n",
       " 'svm': 314,\n",
       " 'signals': 315,\n",
       " 'neuronal': 316,\n",
       " 'global': 317,\n",
       " 'biomedical': 318,\n",
       " 'classifiers': 319,\n",
       " 'cortical': 320,\n",
       " 'digital': 321,\n",
       " 'that': 322,\n",
       " 'language': 323,\n",
       " 'artery': 324,\n",
       " 'gastric': 325,\n",
       " 'content': 326,\n",
       " 'random': 327,\n",
       " 'feasibility': 328,\n",
       " 'signal': 329,\n",
       " 'annotation': 330,\n",
       " 'task': 331,\n",
       " 'biomarkers': 332,\n",
       " 'bladder': 333,\n",
       " 'resolution': 334,\n",
       " 'inference': 335,\n",
       " 'resection': 336,\n",
       " 'modelling': 337,\n",
       " 'improve': 338,\n",
       " 'type': 339,\n",
       " 'process': 340,\n",
       " 'chronic': 341,\n",
       " 'acid': 342,\n",
       " 'chemical': 343,\n",
       " 'differential': 344,\n",
       " 'literature': 345,\n",
       " 'discrimination': 346,\n",
       " 'urinary': 347,\n",
       " 'remote': 348,\n",
       " 'navigation': 349,\n",
       " 'studies': 350,\n",
       " 'behavior': 351,\n",
       " 'cases': 352,\n",
       " 'peptide': 353,\n",
       " 'vision': 354,\n",
       " 'experimental': 355,\n",
       " 'lesions': 356,\n",
       " 'pathway': 357,\n",
       " 'spectra': 358,\n",
       " 'predictive': 359,\n",
       " 'vivo': 360,\n",
       " 'computed': 361,\n",
       " \"alzheimer's\": 362,\n",
       " 'motion': 363,\n",
       " 'species': 364,\n",
       " 'stage': 365,\n",
       " 'functions': 366,\n",
       " 'heart': 367,\n",
       " 'following': 368,\n",
       " 'mechanisms': 369,\n",
       " 'surface': 370,\n",
       " 'efficient': 371,\n",
       " 'properties': 372,\n",
       " 'transcriptomic': 373,\n",
       " 'similarity': 374,\n",
       " 'preliminary': 375,\n",
       " 'registration': 376,\n",
       " 'identify': 377,\n",
       " 'neurons': 378,\n",
       " 'association': 379,\n",
       " 'whole': 380,\n",
       " 'linear': 381,\n",
       " 'comprehensive': 382,\n",
       " 'arm': 383,\n",
       " 'free': 384,\n",
       " 'survival': 385,\n",
       " 'virus': 386,\n",
       " 'virtual': 387,\n",
       " 'soft': 388,\n",
       " 'interpretation': 389,\n",
       " 'stimulation': 390,\n",
       " 'c': 391,\n",
       " 'pyeloplasty': 392,\n",
       " 'reduction': 393,\n",
       " 'series': 394,\n",
       " 'amino': 395,\n",
       " 'robust': 396,\n",
       " 'hand': 397,\n",
       " 'what': 398,\n",
       " 'critical': 399,\n",
       " 'volume': 400,\n",
       " 'bayesian': 401,\n",
       " 'diseases': 402,\n",
       " 'silico': 403,\n",
       " 'primary': 404,\n",
       " 'pelvic': 405,\n",
       " 'electronic': 406,\n",
       " 'pain': 407,\n",
       " 'web': 408,\n",
       " 'how': 409,\n",
       " 'site': 410,\n",
       " 'theory': 411,\n",
       " 'status': 412,\n",
       " 'dependent': 413,\n",
       " 'simple': 414,\n",
       " 'meta': 415,\n",
       " 'tracking': 416,\n",
       " 'independent': 417,\n",
       " 'force': 418,\n",
       " 'big': 419,\n",
       " 'parameters': 420,\n",
       " 'architecture': 421,\n",
       " 'generation': 422,\n",
       " 'strategy': 423,\n",
       " 'upper': 424,\n",
       " 'bone': 425,\n",
       " 'measurement': 426,\n",
       " 'social': 427,\n",
       " 'environmental': 428,\n",
       " 'towards': 429,\n",
       " 'evolutionary': 430,\n",
       " 'unsupervised': 431,\n",
       " 'curve': 432,\n",
       " 'procedures': 433,\n",
       " 'positive': 434,\n",
       " 'component': 435,\n",
       " 'retrieval': 436,\n",
       " 'under': 437,\n",
       " 'syndrome': 438,\n",
       " 'schizophrenia': 439,\n",
       " 'mechanism': 440,\n",
       " 'problem': 441,\n",
       " 'feedback': 442,\n",
       " 'interface': 443,\n",
       " 'test': 444,\n",
       " 'spectrum': 445,\n",
       " 'fusion': 446,\n",
       " 'video': 447,\n",
       " 'pulmonary': 448,\n",
       " 'spinal': 449,\n",
       " 'skin': 450,\n",
       " 'transform': 451,\n",
       " 'pilot': 452,\n",
       " 'space': 453,\n",
       " 'detecting': 454,\n",
       " 'laser': 455,\n",
       " 'conventional': 456,\n",
       " 'semantic': 457,\n",
       " 'lymph': 458,\n",
       " 'node': 459,\n",
       " 'disorder': 460,\n",
       " 'plant': 461,\n",
       " 'injury': 462,\n",
       " 'relationship': 463,\n",
       " 'correlation': 464,\n",
       " 'scheme': 465,\n",
       " 'speech': 466,\n",
       " 'means': 467,\n",
       " 'classifier': 468,\n",
       " 'distribution': 469,\n",
       " 'sparse': 470,\n",
       " 'semi': 471,\n",
       " 'growth': 472,\n",
       " 'movement': 473,\n",
       " 'pediatric': 474,\n",
       " 'tumors': 475,\n",
       " 'rate': 476,\n",
       " 'population': 477,\n",
       " 'planning': 478,\n",
       " 'repair': 479,\n",
       " 'combining': 480,\n",
       " 'proteome': 481,\n",
       " 'evidence': 482,\n",
       " 'joint': 483,\n",
       " 'set': 484,\n",
       " 'fmri': 485,\n",
       " 'flow': 486,\n",
       " 'concept': 487,\n",
       " 'coding': 488,\n",
       " 'applied': 489,\n",
       " 'contrast': 490,\n",
       " 'children': 491,\n",
       " 'transcriptional': 492,\n",
       " 'not': 493,\n",
       " 'processes': 494,\n",
       " 'vs': 495,\n",
       " 'rehabilitation': 496,\n",
       " 'colorectal': 497,\n",
       " 'radial': 498,\n",
       " 'chinese': 499,\n",
       " 'assembly': 500,\n",
       " 'regulatory': 501,\n",
       " 'general': 502,\n",
       " 'advanced': 503,\n",
       " 'influence': 504,\n",
       " 'limb': 505,\n",
       " 'up': 506,\n",
       " 'cellular': 507,\n",
       " 'platform': 508,\n",
       " 'receptor': 509,\n",
       " 'normal': 510,\n",
       " 'hysterectomy': 511,\n",
       " 'flexible': 512,\n",
       " 'complications': 513,\n",
       " 'secondary': 514,\n",
       " 'nerve': 515,\n",
       " 'throughput': 516,\n",
       " 'one': 517,\n",
       " 'life': 518,\n",
       " 'understanding': 519,\n",
       " 'across': 520,\n",
       " 'gait': 521,\n",
       " 'microrna': 522,\n",
       " 'domain': 523,\n",
       " 'sensing': 524,\n",
       " 'action': 525,\n",
       " 'prospective': 526,\n",
       " 'pathology': 527,\n",
       " 'laboratory': 528,\n",
       " 'randomized': 529,\n",
       " 'plasticity': 530,\n",
       " 'pet': 531,\n",
       " 'search': 532,\n",
       " 'ecg': 533,\n",
       " 'density': 534,\n",
       " '3': 535,\n",
       " 'programming': 536,\n",
       " 'mouse': 537,\n",
       " 'without': 538,\n",
       " 'robotically': 539,\n",
       " 'l': 540,\n",
       " 'production': 541,\n",
       " 'practice': 542,\n",
       " 'kernel': 543,\n",
       " 'hierarchical': 544,\n",
       " 'transcription': 545,\n",
       " 'biology': 546,\n",
       " 'fast': 547,\n",
       " 'matter': 548,\n",
       " 'responses': 549,\n",
       " 'pressure': 550,\n",
       " 'genomic': 551,\n",
       " 'left': 552,\n",
       " 'base': 553,\n",
       " 'microscopy': 554,\n",
       " 'transfer': 555,\n",
       " 'inspired': 556,\n",
       " 'regulation': 557,\n",
       " 'synaptic': 558,\n",
       " 'rat': 559,\n",
       " 'mode': 560,\n",
       " 'head': 561,\n",
       " 'maps': 562,\n",
       " 'postoperative': 563,\n",
       " 'parallel': 564,\n",
       " 'weighted': 565,\n",
       " 'factor': 566,\n",
       " 'challenges': 567,\n",
       " 'texture': 568,\n",
       " 'differentiation': 569,\n",
       " 'size': 570,\n",
       " 'precision': 571,\n",
       " 'urology': 572,\n",
       " 'resistance': 573,\n",
       " 'does': 574,\n",
       " 'software': 575,\n",
       " 'cross': 576,\n",
       " 'relationships': 577,\n",
       " 'proteomics': 578,\n",
       " 'perfusion': 579,\n",
       " 'arabidopsis': 580,\n",
       " 'perioperative': 581,\n",
       " 'microarray': 582,\n",
       " 'neck': 583,\n",
       " 'coupled': 584,\n",
       " 'class': 585,\n",
       " 'ii': 586,\n",
       " 'interactive': 587,\n",
       " 'connectivity': 588,\n",
       " 'radiology': 589,\n",
       " 'dissection': 590,\n",
       " 'rule': 591,\n",
       " 'proteomic': 592,\n",
       " 'tools': 593,\n",
       " 'da': 594,\n",
       " 'dose': 595,\n",
       " 'be': 596,\n",
       " 'program': 597,\n",
       " 'laparoscopy': 598,\n",
       " 'organizing': 599,\n",
       " 'engineering': 600,\n",
       " 'neuro': 601,\n",
       " 'hyperspectral': 602,\n",
       " 'investigation': 603,\n",
       " 'profile': 604,\n",
       " 'myocardial': 605,\n",
       " 'cerebral': 606,\n",
       " 'nodules': 607,\n",
       " 'hospital': 608,\n",
       " 'combination': 609,\n",
       " 'disorders': 610,\n",
       " 't': 611,\n",
       " 'age': 612,\n",
       " '5': 613,\n",
       " 'measurements': 614,\n",
       " 'integrating': 615,\n",
       " 'cerebellar': 616,\n",
       " 'perspective': 617,\n",
       " 'metabolic': 618,\n",
       " 'i': 619,\n",
       " 'characteristics': 620,\n",
       " 'safety': 621,\n",
       " 'sleep': 622,\n",
       " 'ablation': 623,\n",
       " 'malignant': 624,\n",
       " 'signatures': 625,\n",
       " 'wavelet': 626,\n",
       " 'lesion': 627,\n",
       " 'mice': 628,\n",
       " 'insights': 629,\n",
       " 'b': 630,\n",
       " 'computing': 631,\n",
       " 'logistic': 632,\n",
       " 'sensory': 633,\n",
       " 'radiotherapy': 634,\n",
       " 'testing': 635,\n",
       " 'thyroidectomy': 636,\n",
       " 'cervical': 637,\n",
       " 'auditory': 638,\n",
       " 'spiking': 639,\n",
       " 'biomarker': 640,\n",
       " 'tree': 641,\n",
       " 'movements': 642,\n",
       " 'rats': 643,\n",
       " 'index': 644,\n",
       " 'scoring': 645,\n",
       " 'online': 646,\n",
       " 'effective': 647,\n",
       " 'device': 648,\n",
       " 'source': 649,\n",
       " 'abdominal': 650,\n",
       " 'vascular': 651,\n",
       " 'construction': 652,\n",
       " 'assistance': 653,\n",
       " 'bioinformatics': 654,\n",
       " 'view': 655,\n",
       " 'white': 656,\n",
       " 'hippocampal': 657,\n",
       " 'spectrometry': 658,\n",
       " 'map': 659,\n",
       " 'structures': 660,\n",
       " 'among': 661,\n",
       " 'methodology': 662,\n",
       " 'regions': 663,\n",
       " 'perspectives': 664,\n",
       " 'correction': 665,\n",
       " 'problems': 666,\n",
       " 'animal': 667,\n",
       " 'text': 668,\n",
       " 'states': 669,\n",
       " 'inhibition': 670,\n",
       " 'serum': 671,\n",
       " 'mammography': 672,\n",
       " 'diverse': 673,\n",
       " 'toward': 674,\n",
       " 'haptic': 675,\n",
       " 'ovarian': 676,\n",
       " 'atrial': 677,\n",
       " 'post': 678,\n",
       " 'overview': 679,\n",
       " 'part': 680,\n",
       " 'line': 681,\n",
       " 'radiation': 682,\n",
       " 'about': 683,\n",
       " 'vinci': 684,\n",
       " 'order': 685,\n",
       " 'advances': 686,\n",
       " 'community': 687,\n",
       " 'failure': 688,\n",
       " 'power': 689,\n",
       " 'region': 690,\n",
       " 'matrix': 691,\n",
       " 'recent': 692,\n",
       " 'pancreatic': 693,\n",
       " 'access': 694,\n",
       " 'propagation': 695,\n",
       " 'measures': 696,\n",
       " 'reports': 697,\n",
       " 'cost': 698,\n",
       " 'exploring': 699,\n",
       " 'diabetes': 700,\n",
       " 'thyroid': 701,\n",
       " 'applying': 702,\n",
       " 'fully': 703,\n",
       " 'compounds': 704,\n",
       " 'visualization': 705,\n",
       " 'muscle': 706,\n",
       " 'impairment': 707,\n",
       " 'evaluating': 708,\n",
       " 'surgeon': 709,\n",
       " 'server': 710,\n",
       " 'technical': 711,\n",
       " 'computation': 712,\n",
       " 'stem': 713,\n",
       " 'prognosis': 714,\n",
       " 'noise': 715,\n",
       " 'over': 716,\n",
       " 'methylation': 717,\n",
       " 'hiv': 718,\n",
       " 'position': 719,\n",
       " 'survey': 720,\n",
       " 'implementation': 721,\n",
       " 'acquisition': 722,\n",
       " 'implications': 723,\n",
       " 'central': 724,\n",
       " 'imagery': 725,\n",
       " 'recovery': 726,\n",
       " 'benign': 727,\n",
       " 'standard': 728,\n",
       " 'samples': 729,\n",
       " 'least': 730,\n",
       " 'mechanical': 731,\n",
       " 'quantification': 732,\n",
       " 'rice': 733,\n",
       " 'stability': 734,\n",
       " 'bypass': 735,\n",
       " 'soil': 736,\n",
       " 'sensor': 737,\n",
       " 'cord': 738,\n",
       " 'infection': 739,\n",
       " 'sensitivity': 740,\n",
       " 'swarm': 741,\n",
       " 'color': 742,\n",
       " 'environment': 743,\n",
       " 'chest': 744,\n",
       " 'optimizing': 745,\n",
       " 'kidney': 746,\n",
       " 'composition': 747,\n",
       " 'relation': 748,\n",
       " 'trends': 749,\n",
       " 'physical': 750,\n",
       " 'eye': 751,\n",
       " 'derived': 752,\n",
       " 'distal': 753,\n",
       " 'objective': 754,\n",
       " 'x': 755,\n",
       " 'therapeutic': 756,\n",
       " 'autonomous': 757,\n",
       " 'adult': 758,\n",
       " 'humans': 759,\n",
       " 'sample': 760,\n",
       " 'signature': 761,\n",
       " 'major': 762,\n",
       " 'needle': 763,\n",
       " 'endometrial': 764,\n",
       " 'frequency': 765,\n",
       " 'records': 766,\n",
       " 'subcellular': 767,\n",
       " 'stochastic': 768,\n",
       " 'co': 769,\n",
       " 'solid': 770,\n",
       " 'autism': 771,\n",
       " 'analyses': 772,\n",
       " 'follow': 773,\n",
       " 'strategies': 774,\n",
       " 'associations': 775,\n",
       " 'multivariate': 776,\n",
       " 'art': 777,\n",
       " 'error': 778,\n",
       " 'manipulation': 779,\n",
       " 'exploration': 780,\n",
       " 'differences': 781,\n",
       " 'dual': 782,\n",
       " 'comparing': 783,\n",
       " 'we': 784,\n",
       " 'stereotactic': 785,\n",
       " 'probabilistic': 786,\n",
       " 'organic': 787,\n",
       " 'toxicity': 788,\n",
       " 'layer': 789,\n",
       " 'assessing': 790,\n",
       " 'year': 791,\n",
       " 'staging': 792,\n",
       " 'like': 793,\n",
       " 'fetal': 794,\n",
       " 'ann': 795,\n",
       " 'energy': 796,\n",
       " 'reference': 797,\n",
       " 'used': 798,\n",
       " 'interfaces': 799,\n",
       " 'targets': 800,\n",
       " 'recurrence': 801,\n",
       " 'knee': 802,\n",
       " 'markers': 803,\n",
       " 'rectal': 804,\n",
       " 'efficacy': 805,\n",
       " 'end': 806,\n",
       " 'healthcare': 807,\n",
       " 'qsar': 808,\n",
       " 'area': 809,\n",
       " 'efficiency': 810,\n",
       " 'highly': 811,\n",
       " 'mrna': 812,\n",
       " 'hip': 813,\n",
       " 'vitro': 814,\n",
       " 'computerized': 815,\n",
       " 'progression': 816,\n",
       " 'particle': 817,\n",
       " 'cholecystectomy': 818,\n",
       " 'within': 819,\n",
       " 'operative': 820,\n",
       " 'nursing': 821,\n",
       " 'mediated': 822,\n",
       " 'guidance': 823,\n",
       " 'matching': 824,\n",
       " 'involved': 825,\n",
       " 'phenotype': 826,\n",
       " 'complexity': 827,\n",
       " 'simultaneous': 828,\n",
       " 'shape': 829,\n",
       " 'sparing': 830,\n",
       " 'event': 831,\n",
       " 'it': 832,\n",
       " 'respiratory': 833,\n",
       " 'improves': 834,\n",
       " 'candidate': 835,\n",
       " 'distributed': 836,\n",
       " 'ray': 837,\n",
       " 'perception': 838,\n",
       " 'fluorescence': 839,\n",
       " 'datasets': 840,\n",
       " 'yeast': 841,\n",
       " 'family': 842,\n",
       " 'predictions': 843,\n",
       " 'analytics': 844,\n",
       " 'do': 845,\n",
       " 'bacterial': 846,\n",
       " 'building': 847,\n",
       " 'identifies': 848,\n",
       " 'integrative': 849,\n",
       " 'individual': 850,\n",
       " 'weight': 851,\n",
       " 'walking': 852,\n",
       " 'squares': 853,\n",
       " 'skills': 854,\n",
       " 'biochemical': 855,\n",
       " 'adaptation': 856,\n",
       " 'presence': 857,\n",
       " 'inhibitors': 858,\n",
       " 'reality': 859,\n",
       " 'hepatic': 860,\n",
       " 'k': 861,\n",
       " 'splicing': 862,\n",
       " 'behavioral': 863,\n",
       " 'peptides': 864,\n",
       " 'noninvasive': 865,\n",
       " 'undergoing': 866,\n",
       " 'sets': 867,\n",
       " 'improvement': 868,\n",
       " 'negative': 869,\n",
       " 'spectral': 870,\n",
       " 'encoding': 871,\n",
       " 'forecasting': 872,\n",
       " 'reasoning': 873,\n",
       " 'diffusion': 874,\n",
       " 'rnas': 875,\n",
       " 'radiosurgery': 876,\n",
       " 'red': 877,\n",
       " 'evoked': 878,\n",
       " 'oral': 879,\n",
       " 'minimal': 880,\n",
       " 'value': 881,\n",
       " 'nir': 882,\n",
       " 'retinal': 883,\n",
       " 'key': 884,\n",
       " 'unit': 885,\n",
       " 'anti': 886,\n",
       " 'common': 887,\n",
       " 'alternative': 888,\n",
       " 'bilateral': 889,\n",
       " 'continuous': 890,\n",
       " 'men': 891,\n",
       " 'reveal': 892,\n",
       " 'women': 893,\n",
       " 'signaling': 894,\n",
       " \"parkinson's\": 895,\n",
       " 'events': 896,\n",
       " 'glaucoma': 897,\n",
       " 'emerging': 898,\n",
       " 'continence': 899,\n",
       " 'masses': 900,\n",
       " 'retropubic': 901,\n",
       " 'healthy': 902,\n",
       " 'fields': 903,\n",
       " 'micro': 904,\n",
       " 'drugs': 905,\n",
       " 'classifying': 906,\n",
       " 'prognostic': 907,\n",
       " 'face': 908,\n",
       " 'array': 909,\n",
       " 'discriminant': 910,\n",
       " 'preoperative': 911,\n",
       " 'various': 912,\n",
       " 'tract': 913,\n",
       " 'lateral': 914,\n",
       " 'group': 915,\n",
       " 'removal': 916,\n",
       " 'hepatitis': 917,\n",
       " 'antigen': 918,\n",
       " 'simulated': 919,\n",
       " 'angiography': 920,\n",
       " 'phenotyping': 921,\n",
       " 'pseudo': 922,\n",
       " 'technologies': 923,\n",
       " 'update': 924,\n",
       " 'loop': 925,\n",
       " 'ureteral': 926,\n",
       " 'temperature': 927,\n",
       " 'nuclear': 928,\n",
       " 'seq': 929,\n",
       " 'wave': 930,\n",
       " 'evolving': 931,\n",
       " 'variables': 932,\n",
       " 'intensive': 933,\n",
       " 'tasks': 934,\n",
       " 'mobile': 935,\n",
       " 'metastasis': 936,\n",
       " 'four': 937,\n",
       " 'higher': 938,\n",
       " 'point': 939,\n",
       " 'european': 940,\n",
       " 'representations': 941,\n",
       " 'facial': 942,\n",
       " 'measuring': 943,\n",
       " 'developmental': 944,\n",
       " 'binary': 945,\n",
       " 'decomposition': 946,\n",
       " 'variable': 947,\n",
       " 'sacrocolpopexy': 948,\n",
       " 'reduced': 949,\n",
       " 'trees': 950,\n",
       " 'release': 951,\n",
       " 'maize': 952,\n",
       " 'china': 953,\n",
       " 'object': 954,\n",
       " 'food': 955,\n",
       " 'next': 956,\n",
       " 'teaching': 957,\n",
       " 'contact': 958,\n",
       " 'oncologic': 959,\n",
       " 'filter': 960,\n",
       " 'neuroscience': 961,\n",
       " 'heterogeneous': 962,\n",
       " 'prior': 963,\n",
       " 'ligand': 964,\n",
       " 'backpropagation': 965,\n",
       " 'guide': 966,\n",
       " 'channel': 967,\n",
       " 'conditions': 968,\n",
       " 'differentially': 969,\n",
       " 'expressed': 970,\n",
       " 'intraoperative': 971,\n",
       " 'modified': 972,\n",
       " 'totally': 973,\n",
       " 'education': 974,\n",
       " 'transoral': 975,\n",
       " 'interventions': 976,\n",
       " 'genomes': 977,\n",
       " 'ai': 978,\n",
       " 'center': 979,\n",
       " 'selective': 980,\n",
       " 'potentials': 981,\n",
       " 'all': 982,\n",
       " 'prolapse': 983,\n",
       " 'revealed': 984,\n",
       " 'multimodal': 985,\n",
       " 'variants': 986,\n",
       " 'severity': 987,\n",
       " 'other': 988,\n",
       " 'placement': 989,\n",
       " 'organ': 990,\n",
       " 'change': 991,\n",
       " 'adrenalectomy': 992,\n",
       " 'cohort': 993,\n",
       " 'physiological': 994,\n",
       " 'enhancing': 995,\n",
       " 'rates': 996,\n",
       " 'databases': 997,\n",
       " 'structured': 998,\n",
       " 'immune': 999,\n",
       " 'gradient': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahlem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Fonction permettant de charger un embedding \n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_glove_embeddings(fp, embedding_dim, include_empty_char=True):\n",
    "    \"\"\"\n",
    "    Loads pre-trained word embeddings (GloVe embeddings)\n",
    "        Inputs: - fp: filepath of pre-trained glove embeddings\n",
    "                - embedding_dim: dimension of each vector embedding\n",
    "                - generate_matrix: whether to generate an embedding matrix\n",
    "        Outputs:\n",
    "                - word2coefs: Dictionary. Word to its corresponding coefficients\n",
    "                - word2index: Dictionary. Word to word-index\n",
    "                - embedding_matrix: Embedding matrix for Keras Embedding layer\n",
    "    \"\"\"\n",
    "    # First, build the \"word2coefs\" and \"word2index\"\n",
    "    word2coefs = {} # word to its corresponding coefficients\n",
    "    word2index = {} # word to word-index\n",
    "    with open(fp) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            try:\n",
    "                data = [x.strip().lower() for x in line.split()]\n",
    "                word = data[0]\n",
    "                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')\n",
    "                word2coefs[word] = coefs\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = len(word2index)\n",
    "            except Exception as e:\n",
    "                print('Exception occurred in `load_glove_embeddings`:', e)\n",
    "                continue\n",
    "        # End of for loop.\n",
    "    # End of with open\n",
    "    if include_empty_char:\n",
    "        word2index[''] = len(word2index)\n",
    "    # Second, build the \"embedding_matrix\"\n",
    "    # Words not found in embedding index will be all-zeros. Hence, the \"+1\".\n",
    "    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in word2index.items():\n",
    "        embedding_vec = word2coefs.get(word)\n",
    "        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:\n",
    "            embedding_matrix[idx] = np.asarray(embedding_vec)\n",
    "    # return word2coefs, word2index, embedding_matrix\n",
    "    return word2index, np.asarray(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les embeddings √† l'aide de la fonction load_glove_embeddings\n",
    "\n",
    "word2index, embedding_matrix = load_glove_embeddings('glove.6B.50d.txt', embedding_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201534"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an improved kernel based extreme learning machine for robot execution failures.\n",
      "[   29  2338 21566   243  3828  2741  2358    10  9247  4415  7693     2]\n",
      "[ 6707 13867  2233     6   426  2022  2817     2]\n"
     ]
    }
   ],
   "source": [
    "# ecrire une fonction de tokenization custom pour preprocesser les textes\n",
    "unknown_words = set()\n",
    "known_words = set()\n",
    "\n",
    "def custom_tokenize(doc):\n",
    "  res = []\n",
    "  for ex in word_tokenize(doc): \n",
    "    if ex not in word2index.keys():\n",
    "      res.append(word2index['unk'])\n",
    "      unknown_words.add(ex)\n",
    "    else:\n",
    "      res.append(word2index[ex])\n",
    "      known_words.add(ex)\n",
    "  return np.array(res)\n",
    "    \n",
    "# Encoder les textes avec la fonction custom\n",
    "X_train_glove = [custom_tokenize(x) for x in x_train]\n",
    "print(x_train[0])\n",
    "print(X_train_glove[0])\n",
    "\n",
    "X_test_glove = [custom_tokenize(x) for x in x_test]\n",
    "print(X_test_glove[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Glove, there is 4592 unknown words and 11324 known words.\n"
     ]
    }
   ],
   "source": [
    "print(f\"With Glove, there is {len(unknown_words)} unknown words and {len(known_words)} known words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding des sequences\n",
    "\n",
    "X_train_glove = tf.keras.preprocessing.sequence.pad_sequences(X_train_glove, maxlen = 40, truncating='post')\n",
    "X_test_glove = tf.keras.preprocessing.sequence.pad_sequences(X_test_glove, maxlen = 40, truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Mod√®le basique\n",
    "## Embedding non pr√©entrain√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 39, 32)            8224      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 93)                11997     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 93)                0         \n",
      "=================================================================\n",
      "Total params: 1,304,445\n",
      "Trainable params: 1,304,445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential() # mod√®le s√©quentiel\n",
    "model.add(tf.keras.layers.Embedding(10000, embed_dim,input_length = 40))# couche d'embedding de taille 128\n",
    "model.add(tf.keras.layers.Conv1D(32,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Compiler le mod√®le \n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "# Afficher le summary du mod√®le\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 3.9159 - acc: 0.1690 - val_loss: 3.4554 - val_acc: 0.1880\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 3.3196 - acc: 0.2320 - val_loss: 3.1856 - val_acc: 0.2675\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 3.0286 - acc: 0.2970 - val_loss: 2.9929 - val_acc: 0.3130\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 2.7075 - acc: 0.3621 - val_loss: 2.8633 - val_acc: 0.3310\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 2.3373 - acc: 0.4282 - val_loss: 2.7857 - val_acc: 0.3510\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 1.9620 - acc: 0.5084 - val_loss: 2.8051 - val_acc: 0.3535\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 1.6304 - acc: 0.5867 - val_loss: 2.9213 - val_acc: 0.3315\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 1.3204 - acc: 0.6665 - val_loss: 3.0590 - val_acc: 0.3280\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 1.0833 - acc: 0.7344 - val_loss: 3.2589 - val_acc: 0.3265\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.8676 - acc: 0.7879 - val_loss: 3.4859 - val_acc: 0.3205\n"
     ]
    }
   ],
   "source": [
    "# Fitter le mod√®le \n",
    "\n",
    "history = model.fit(x_train_seq, y_train_id, batch_size = batch_size, epochs = 10, validation_data=(x_test_seq, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualiser\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 75us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.485855577468872, 0.3205]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Mod√®le basique\n",
    "## Embedding GloVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 39, 32)            3232      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 93)                11997     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 93)                0         \n",
      "=================================================================\n",
      "Total params: 20,019,503\n",
      "Trainable params: 20,019,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Entrainer un mod√®le en chargeant les poids des embeddings dans le layer Embedding\n",
    "\n",
    "#embed_dim = 50\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 40,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True))\n",
    "model.add(tf.keras.layers.Conv1D(32,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=adam, metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 3.8672 - acc: 0.1354 - val_loss: 3.4002 - val_acc: 0.2155\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 3.3837 - acc: 0.2245 - val_loss: 3.1642 - val_acc: 0.2650\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 3.1539 - acc: 0.2605 - val_loss: 3.0185 - val_acc: 0.2725\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 2.9960 - acc: 0.2899 - val_loss: 2.9066 - val_acc: 0.2905\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.8418 - acc: 0.3086 - val_loss: 2.7957 - val_acc: 0.3055\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 2.7185 - acc: 0.3284 - val_loss: 2.7085 - val_acc: 0.3145\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 2.5868 - acc: 0.3520 - val_loss: 2.6429 - val_acc: 0.3290\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 2.4668 - acc: 0.3732 - val_loss: 2.5899 - val_acc: 0.3350\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 2.3697 - acc: 0.3915 - val_loss: 2.5686 - val_acc: 0.3385\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 2.2819 - acc: 0.4041 - val_loss: 2.5413 - val_acc: 0.3400\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 10, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 76us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.541342189788818, 0.34]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Mod√®le basique, Changement hyperparam√®tres\n",
    "## Embedding non pr√©entrain√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 4.4543 - acc: 0.1118 - val_loss: 4.3578 - val_acc: 0.1880\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 4.1809 - acc: 0.1878 - val_loss: 3.9181 - val_acc: 0.1880\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 3.7160 - acc: 0.1876 - val_loss: 3.5543 - val_acc: 0.1880\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 3.5791 - acc: 0.1870 - val_loss: 3.5007 - val_acc: 0.1880\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 3.5295 - acc: 0.1871 - val_loss: 3.4690 - val_acc: 0.1880\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 3.5086 - acc: 0.1876 - val_loss: 3.4440 - val_acc: 0.1885\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 3.4657 - acc: 0.2001 - val_loss: 3.4049 - val_acc: 0.2175\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 3.4125 - acc: 0.2246 - val_loss: 3.3494 - val_acc: 0.2305\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.3532 - acc: 0.2407 - val_loss: 3.2958 - val_acc: 0.2485\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.2991 - acc: 0.2490 - val_loss: 3.2569 - val_acc: 0.2585\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.2531 - acc: 0.2544 - val_loss: 3.2241 - val_acc: 0.2640\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.2160 - acc: 0.2621 - val_loss: 3.1941 - val_acc: 0.2645\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.1683 - acc: 0.2682 - val_loss: 3.1663 - val_acc: 0.2655\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.1293 - acc: 0.2781 - val_loss: 3.1357 - val_acc: 0.2770\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.0801 - acc: 0.2871 - val_loss: 3.1042 - val_acc: 0.2840\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 3.0373 - acc: 0.3028 - val_loss: 3.0702 - val_acc: 0.2965\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 2.9854 - acc: 0.3161 - val_loss: 3.0361 - val_acc: 0.3045\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 2.9182 - acc: 0.3291 - val_loss: 3.0025 - val_acc: 0.3100\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 2.8631 - acc: 0.3319 - val_loss: 2.9715 - val_acc: 0.3165\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.8040 - acc: 0.3449 - val_loss: 2.9420 - val_acc: 0.3200\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.7387 - acc: 0.3575 - val_loss: 2.9145 - val_acc: 0.3250\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 2.6810 - acc: 0.3783 - val_loss: 2.8922 - val_acc: 0.3245\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 2.6139 - acc: 0.3954 - val_loss: 2.8675 - val_acc: 0.3305\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 2.5672 - acc: 0.4073 - val_loss: 2.8468 - val_acc: 0.3370\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 2.4992 - acc: 0.4220 - val_loss: 2.8291 - val_acc: 0.3445\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.4436 - acc: 0.4363 - val_loss: 2.8133 - val_acc: 0.3470\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 2.3850 - acc: 0.4440 - val_loss: 2.8003 - val_acc: 0.3500\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.3282 - acc: 0.4552 - val_loss: 2.7893 - val_acc: 0.3530\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.2705 - acc: 0.4664 - val_loss: 2.7815 - val_acc: 0.3550\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.2152 - acc: 0.4776 - val_loss: 2.7754 - val_acc: 0.3590\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 2.1656 - acc: 0.4875 - val_loss: 2.7730 - val_acc: 0.3590\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.1109 - acc: 0.4988 - val_loss: 2.7692 - val_acc: 0.3595\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 2.0654 - acc: 0.5088 - val_loss: 2.7678 - val_acc: 0.3640\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 2.0128 - acc: 0.5175 - val_loss: 2.7696 - val_acc: 0.3630\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 1.9643 - acc: 0.5285 - val_loss: 2.7717 - val_acc: 0.3650\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 1.9210 - acc: 0.5386 - val_loss: 2.7745 - val_acc: 0.3620\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.8777 - acc: 0.5459 - val_loss: 2.7798 - val_acc: 0.3635\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.8359 - acc: 0.5586 - val_loss: 2.7878 - val_acc: 0.3615\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.7926 - acc: 0.5664 - val_loss: 2.7943 - val_acc: 0.3600\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.7498 - acc: 0.5760 - val_loss: 2.8026 - val_acc: 0.3630\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.7027 - acc: 0.5919 - val_loss: 2.8103 - val_acc: 0.3625\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 1.6607 - acc: 0.6031 - val_loss: 2.8196 - val_acc: 0.3605\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.6247 - acc: 0.6086 - val_loss: 2.8324 - val_acc: 0.3595\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.5821 - acc: 0.6180 - val_loss: 2.8457 - val_acc: 0.3615\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 1.5434 - acc: 0.6243 - val_loss: 2.8605 - val_acc: 0.3625\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.5055 - acc: 0.6377 - val_loss: 2.8742 - val_acc: 0.3625\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.4647 - acc: 0.6495 - val_loss: 2.8867 - val_acc: 0.3650\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.4177 - acc: 0.6620 - val_loss: 2.9050 - val_acc: 0.3635\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.3910 - acc: 0.6663 - val_loss: 2.9193 - val_acc: 0.3675\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 1.3538 - acc: 0.6731 - val_loss: 2.9388 - val_acc: 0.3705\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential() # mod√®le s√©quentiel\n",
    "model.add(tf.keras.layers.Embedding(10000, embed_dim,input_length = 40))# couche d'embedding de taille \n",
    "model.add(tf.keras.layers.Conv1D(256,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Compiler le mod√®le \n",
    "adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=adam ,metrics = ['accuracy'])\n",
    "# Fitter le mod√®le \n",
    "history = model.fit(x_train_seq, y_train_id, batch_size = batch_size, epochs = 50, validation_data=(x_test_seq, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.938798152923584, 0.3705]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Mod√®le basique, Changement hyperparam√®tres\n",
    "## Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 4.1738 - acc: 0.1159 - val_loss: 3.7418 - val_acc: 0.1880\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 3.7465 - acc: 0.1796 - val_loss: 3.5511 - val_acc: 0.1880\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 3.6235 - acc: 0.1878 - val_loss: 3.4553 - val_acc: 0.1930\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 3.5207 - acc: 0.2048 - val_loss: 3.3781 - val_acc: 0.2200\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 3.4455 - acc: 0.2174 - val_loss: 3.3054 - val_acc: 0.2445\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 3.3538 - acc: 0.2324 - val_loss: 3.2388 - val_acc: 0.2610\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 3.2778 - acc: 0.2496 - val_loss: 3.1778 - val_acc: 0.2665\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 3.2112 - acc: 0.2525 - val_loss: 3.1233 - val_acc: 0.2675\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 3.1574 - acc: 0.2605 - val_loss: 3.0769 - val_acc: 0.2765\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 3.1068 - acc: 0.2769 - val_loss: 3.0341 - val_acc: 0.2770\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 3.0546 - acc: 0.2815 - val_loss: 2.9945 - val_acc: 0.2860\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 3.0017 - acc: 0.2845 - val_loss: 2.9572 - val_acc: 0.2920\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 2.9690 - acc: 0.2985 - val_loss: 2.9223 - val_acc: 0.2965\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 2.9108 - acc: 0.3073 - val_loss: 2.8905 - val_acc: 0.3010\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2.8831 - acc: 0.3109 - val_loss: 2.8575 - val_acc: 0.3015\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.8481 - acc: 0.3204 - val_loss: 2.8296 - val_acc: 0.3165\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 2.8256 - acc: 0.3182 - val_loss: 2.7996 - val_acc: 0.3165\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 2.7776 - acc: 0.3287 - val_loss: 2.7751 - val_acc: 0.3245\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 2.7475 - acc: 0.3327 - val_loss: 2.7512 - val_acc: 0.3250\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.7198 - acc: 0.3377 - val_loss: 2.7253 - val_acc: 0.3300\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 2.6839 - acc: 0.3405 - val_loss: 2.7066 - val_acc: 0.3260\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2.6553 - acc: 0.3465 - val_loss: 2.6880 - val_acc: 0.3360\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 2.6365 - acc: 0.3515 - val_loss: 2.6691 - val_acc: 0.3375\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 2.6052 - acc: 0.3581 - val_loss: 2.6487 - val_acc: 0.3365\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 2.5784 - acc: 0.3626 - val_loss: 2.6334 - val_acc: 0.3350\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2.5584 - acc: 0.3588 - val_loss: 2.6203 - val_acc: 0.3415\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 2.5322 - acc: 0.3612 - val_loss: 2.6063 - val_acc: 0.3410\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 2.5050 - acc: 0.3746 - val_loss: 2.5899 - val_acc: 0.3435\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.4822 - acc: 0.3762 - val_loss: 2.5786 - val_acc: 0.3450\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 2.4769 - acc: 0.3779 - val_loss: 2.5647 - val_acc: 0.3445\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 2.4556 - acc: 0.3805 - val_loss: 2.5518 - val_acc: 0.3465\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2.4261 - acc: 0.3815 - val_loss: 2.5420 - val_acc: 0.3490\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 2.4060 - acc: 0.3869 - val_loss: 2.5292 - val_acc: 0.3490\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.3828 - acc: 0.3995 - val_loss: 2.5215 - val_acc: 0.3465\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 2.3742 - acc: 0.3977 - val_loss: 2.5097 - val_acc: 0.3520\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 2.3539 - acc: 0.4024 - val_loss: 2.5005 - val_acc: 0.3535\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.3401 - acc: 0.4007 - val_loss: 2.4918 - val_acc: 0.3560\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 2.3217 - acc: 0.4050 - val_loss: 2.4846 - val_acc: 0.3570\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2.3070 - acc: 0.4047 - val_loss: 2.4740 - val_acc: 0.3575\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 2.2800 - acc: 0.4115 - val_loss: 2.4646 - val_acc: 0.3565\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 2.2642 - acc: 0.4133 - val_loss: 2.4576 - val_acc: 0.3550\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 2.2480 - acc: 0.4165 - val_loss: 2.4518 - val_acc: 0.3570\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 2.2282 - acc: 0.4263 - val_loss: 2.4450 - val_acc: 0.3555\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.2106 - acc: 0.4229 - val_loss: 2.4345 - val_acc: 0.3560\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.1909 - acc: 0.4319 - val_loss: 2.4277 - val_acc: 0.3560\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 2.1802 - acc: 0.4353 - val_loss: 2.4222 - val_acc: 0.3575\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 2.1677 - acc: 0.4355 - val_loss: 2.4154 - val_acc: 0.3605\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 2.1413 - acc: 0.4440 - val_loss: 2.4097 - val_acc: 0.3630\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 2.1348 - acc: 0.4446 - val_loss: 2.4056 - val_acc: 0.3640\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 2.1102 - acc: 0.4516 - val_loss: 2.3982 - val_acc: 0.3645\n"
     ]
    }
   ],
   "source": [
    "#embed_dim = 50\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential() # mod√®le s√©quentiel\n",
    "model.add(tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 40,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True))# couche d'embedding de taille \n",
    "model.add(tf.keras.layers.Conv1D(256,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Compiler le mod√®le \n",
    "adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=adam ,metrics = ['accuracy'])\n",
    "# Fitter le mod√®le \n",
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 50, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 90us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3543439178466796, 0.377]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Mod√®le CNN multi-channel\n",
    "## Embedding non pr√©entrain√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/45\n",
      "8000/8000 [==============================] - 3s 386us/step - loss: 4.2620 - acc: 0.1373 - val_loss: 3.9116 - val_acc: 0.1880\n",
      "Epoch 2/45\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 3.6128 - acc: 0.1894 - val_loss: 3.4749 - val_acc: 0.1880\n",
      "Epoch 3/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 3.4284 - acc: 0.1933 - val_loss: 3.4192 - val_acc: 0.1955\n",
      "Epoch 4/45\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 3.3641 - acc: 0.2151 - val_loss: 3.3702 - val_acc: 0.2205\n",
      "Epoch 5/45\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 3.2994 - acc: 0.2333 - val_loss: 3.3141 - val_acc: 0.2360\n",
      "Epoch 6/45\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 3.2293 - acc: 0.2449 - val_loss: 3.2620 - val_acc: 0.2515\n",
      "Epoch 7/45\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 3.1709 - acc: 0.2509 - val_loss: 3.2208 - val_acc: 0.2520\n",
      "Epoch 8/45\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 3.1107 - acc: 0.2574 - val_loss: 3.1860 - val_acc: 0.2565\n",
      "Epoch 9/45\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 3.0565 - acc: 0.2664 - val_loss: 3.1533 - val_acc: 0.2630\n",
      "Epoch 10/45\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 3.0002 - acc: 0.2794 - val_loss: 3.1199 - val_acc: 0.2700\n",
      "Epoch 11/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 2.9457 - acc: 0.2863 - val_loss: 3.0848 - val_acc: 0.2800\n",
      "Epoch 12/45\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 2.8876 - acc: 0.3046 - val_loss: 3.0478 - val_acc: 0.2860\n",
      "Epoch 13/45\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 2.8183 - acc: 0.3173 - val_loss: 3.0091 - val_acc: 0.2960\n",
      "Epoch 14/45\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 2.7486 - acc: 0.3357 - val_loss: 2.9697 - val_acc: 0.3020\n",
      "Epoch 15/45\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 2.6858 - acc: 0.3570 - val_loss: 2.9300 - val_acc: 0.3080\n",
      "Epoch 16/45\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 2.6089 - acc: 0.3786 - val_loss: 2.8915 - val_acc: 0.3265\n",
      "Epoch 17/45\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 2.5411 - acc: 0.3971 - val_loss: 2.8564 - val_acc: 0.3340\n",
      "Epoch 18/45\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 2.4667 - acc: 0.4226 - val_loss: 2.8217 - val_acc: 0.3440\n",
      "Epoch 19/45\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 2.3933 - acc: 0.4379 - val_loss: 2.7902 - val_acc: 0.3460\n",
      "Epoch 20/45\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 2.3282 - acc: 0.4580 - val_loss: 2.7606 - val_acc: 0.3570\n",
      "Epoch 21/45\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 2.2543 - acc: 0.4750 - val_loss: 2.7312 - val_acc: 0.3570\n",
      "Epoch 22/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 2.1860 - acc: 0.4862 - val_loss: 2.7070 - val_acc: 0.3685\n",
      "Epoch 23/45\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 2.1132 - acc: 0.5022 - val_loss: 2.6806 - val_acc: 0.3685\n",
      "Epoch 24/45\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 2.0498 - acc: 0.5135 - val_loss: 2.6586 - val_acc: 0.3740\n",
      "Epoch 25/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.9787 - acc: 0.5307 - val_loss: 2.6373 - val_acc: 0.3770\n",
      "Epoch 26/45\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 1.9170 - acc: 0.5461 - val_loss: 2.6178 - val_acc: 0.3785\n",
      "Epoch 27/45\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 1.8440 - acc: 0.5637 - val_loss: 2.6015 - val_acc: 0.3795\n",
      "Epoch 28/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 1.7816 - acc: 0.5761 - val_loss: 2.5858 - val_acc: 0.3800\n",
      "Epoch 29/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 1.7157 - acc: 0.5985 - val_loss: 2.5694 - val_acc: 0.3795\n",
      "Epoch 30/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.6553 - acc: 0.6082 - val_loss: 2.5570 - val_acc: 0.3780\n",
      "Epoch 31/45\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 1.5982 - acc: 0.6254 - val_loss: 2.5450 - val_acc: 0.3820\n",
      "Epoch 32/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.5380 - acc: 0.6479 - val_loss: 2.5365 - val_acc: 0.3865\n",
      "Epoch 33/45\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 1.4833 - acc: 0.6603 - val_loss: 2.5267 - val_acc: 0.3895\n",
      "Epoch 34/45\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 1.4280 - acc: 0.6756 - val_loss: 2.5197 - val_acc: 0.3930\n",
      "Epoch 35/45\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 1.3709 - acc: 0.6929 - val_loss: 2.5138 - val_acc: 0.3945\n",
      "Epoch 36/45\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 1.3176 - acc: 0.7077 - val_loss: 2.5081 - val_acc: 0.3960\n",
      "Epoch 37/45\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 1.2666 - acc: 0.7239 - val_loss: 2.5028 - val_acc: 0.3955\n",
      "Epoch 38/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 1.2148 - acc: 0.7356 - val_loss: 2.5003 - val_acc: 0.3975\n",
      "Epoch 39/45\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 1.1672 - acc: 0.7514 - val_loss: 2.4973 - val_acc: 0.3990\n",
      "Epoch 40/45\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 1.1211 - acc: 0.7649 - val_loss: 2.4966 - val_acc: 0.4005\n",
      "Epoch 41/45\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 1.0758 - acc: 0.7792 - val_loss: 2.4969 - val_acc: 0.4000\n",
      "Epoch 42/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 1.0298 - acc: 0.7899 - val_loss: 2.4973 - val_acc: 0.4005\n",
      "Epoch 43/45\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 0.9923 - acc: 0.7950 - val_loss: 2.4987 - val_acc: 0.3995\n",
      "Epoch 44/45\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.9480 - acc: 0.8097 - val_loss: 2.5011 - val_acc: 0.4040\n",
      "Epoch 45/45\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.9067 - acc: 0.8190 - val_loss: 2.5054 - val_acc: 0.4030\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    filter_sizes = [2, 3, 5]\n",
    "    num_filters = 256\n",
    "    drop = 0.3\n",
    "    MAX_LENGTH=40\n",
    "    MAX_NB_WORDS = 10000\n",
    "    \n",
    "    inputs = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "    embedding = Embedding(input_dim=MAX_NB_WORDS,\n",
    "                                output_dim=embedding_dim,\n",
    "                                input_length=MAX_LENGTH)(inputs)\n",
    "\n",
    "    reshape = Reshape((MAX_LENGTH, embedding_dim, 1))(embedding)\n",
    "    conv_0 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[0], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    conv_1 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[1], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[2], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_0)\n",
    "\n",
    "    maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_1)\n",
    "\n",
    "    maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_2)\n",
    "    concatenated_tensor = Concatenate(axis=1)(\n",
    "        [maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    \n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    \n",
    "    #outpout1=Dense(units=256, activation='relu')(dropout)\n",
    "    #dropout1 = Dropout(drop)(outpout1)\n",
    "    \n",
    "    output = Dense(units=cat_vocab, activation='softmax')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn_model_multi_channel = get_cnn_model()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 45\n",
    "history = cnn_model_multi_channel.fit(x=x_train_seq, \n",
    "                    y=y_train_id, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_test_seq, y_test_id),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 133us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5053614768981936, 0.403]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "cnn_model_multi_channel.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Mod√®le CNN multi-channel\n",
    "## Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    embedding_dim = 50\n",
    "    filter_sizes = [2, 3, 5]\n",
    "    num_filters = 256\n",
    "    drop = 0.3\n",
    "    MAX_LENGTH=40\n",
    "    MAX_NB_WORDS = 10000\n",
    "    \n",
    "    inputs = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "    embedding = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 40,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True)(inputs)\n",
    "\n",
    "    reshape = Reshape((MAX_LENGTH, embedding_dim, 1))(embedding)\n",
    "    conv_0 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[0], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    conv_1 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[1], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[2], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_0)\n",
    "\n",
    "    maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_1)\n",
    "\n",
    "    maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_2)\n",
    "    concatenated_tensor = Concatenate(axis=1)(\n",
    "        [maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    \n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    \n",
    "    #outpout1=Dense(units=256, activation='relu')(dropout)\n",
    "    #dropout1 = Dropout(drop)(outpout1)\n",
    "    \n",
    "    output = Dense(units=cat_vocab, activation='softmax')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_cnn_model()\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/45\n",
      "8000/8000 [==============================] - 2s 254us/step - loss: 4.0113 - acc: 0.1211 - val_loss: 3.4170 - val_acc: 0.2065\n",
      "Epoch 2/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 3.4965 - acc: 0.1895 - val_loss: 3.2945 - val_acc: 0.2355\n",
      "Epoch 3/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 3.3612 - acc: 0.2209 - val_loss: 3.2022 - val_acc: 0.2700\n",
      "Epoch 4/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 3.2472 - acc: 0.2501 - val_loss: 3.1302 - val_acc: 0.2825\n",
      "Epoch 5/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 3.1409 - acc: 0.2699 - val_loss: 3.0645 - val_acc: 0.2895\n",
      "Epoch 6/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 3.0620 - acc: 0.2775 - val_loss: 3.0107 - val_acc: 0.2945\n",
      "Epoch 7/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.9967 - acc: 0.2923 - val_loss: 2.9612 - val_acc: 0.3035\n",
      "Epoch 8/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 2.9165 - acc: 0.3000 - val_loss: 2.9186 - val_acc: 0.3065\n",
      "Epoch 9/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 2.8497 - acc: 0.3121 - val_loss: 2.8788 - val_acc: 0.3120\n",
      "Epoch 10/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.8005 - acc: 0.3264 - val_loss: 2.8424 - val_acc: 0.3185\n",
      "Epoch 11/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.7506 - acc: 0.3332 - val_loss: 2.8084 - val_acc: 0.3255\n",
      "Epoch 12/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 2.6917 - acc: 0.3397 - val_loss: 2.7803 - val_acc: 0.3305\n",
      "Epoch 13/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 2.6528 - acc: 0.3454 - val_loss: 2.7502 - val_acc: 0.3290\n",
      "Epoch 14/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 2.5933 - acc: 0.3605 - val_loss: 2.7225 - val_acc: 0.3370\n",
      "Epoch 15/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 2.5509 - acc: 0.3666 - val_loss: 2.6983 - val_acc: 0.3430\n",
      "Epoch 16/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 2.5154 - acc: 0.3743 - val_loss: 2.6743 - val_acc: 0.3475\n",
      "Epoch 17/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.4775 - acc: 0.3772 - val_loss: 2.6540 - val_acc: 0.3460\n",
      "Epoch 18/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 2.4384 - acc: 0.3805 - val_loss: 2.6343 - val_acc: 0.3540\n",
      "Epoch 19/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.4015 - acc: 0.3925 - val_loss: 2.6142 - val_acc: 0.3560\n",
      "Epoch 20/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.3615 - acc: 0.3977 - val_loss: 2.5974 - val_acc: 0.3570\n",
      "Epoch 21/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.3332 - acc: 0.4091 - val_loss: 2.5806 - val_acc: 0.3580\n",
      "Epoch 22/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 2.2945 - acc: 0.4095 - val_loss: 2.5633 - val_acc: 0.3590\n",
      "Epoch 23/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.2669 - acc: 0.4158 - val_loss: 2.5464 - val_acc: 0.3625\n",
      "Epoch 24/45\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 2.2314 - acc: 0.4219 - val_loss: 2.5350 - val_acc: 0.3630\n",
      "Epoch 25/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.2074 - acc: 0.4271 - val_loss: 2.5209 - val_acc: 0.3635\n",
      "Epoch 26/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.1735 - acc: 0.4355 - val_loss: 2.5085 - val_acc: 0.3700\n",
      "Epoch 27/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 2.1506 - acc: 0.4387 - val_loss: 2.4947 - val_acc: 0.3670\n",
      "Epoch 28/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.1313 - acc: 0.4401 - val_loss: 2.4855 - val_acc: 0.3695\n",
      "Epoch 29/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.0954 - acc: 0.4450 - val_loss: 2.4731 - val_acc: 0.3690\n",
      "Epoch 30/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 2.0731 - acc: 0.4544 - val_loss: 2.4637 - val_acc: 0.3680\n",
      "Epoch 31/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.0388 - acc: 0.4646 - val_loss: 2.4535 - val_acc: 0.3710\n",
      "Epoch 32/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 2.0106 - acc: 0.4665 - val_loss: 2.4432 - val_acc: 0.3735\n",
      "Epoch 33/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.9970 - acc: 0.4705 - val_loss: 2.4324 - val_acc: 0.3750\n",
      "Epoch 34/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.9744 - acc: 0.4746 - val_loss: 2.4267 - val_acc: 0.3740\n",
      "Epoch 35/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.9442 - acc: 0.4830 - val_loss: 2.4172 - val_acc: 0.3750\n",
      "Epoch 36/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.9242 - acc: 0.4900 - val_loss: 2.4098 - val_acc: 0.3750\n",
      "Epoch 37/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.9042 - acc: 0.4954 - val_loss: 2.4009 - val_acc: 0.3785\n",
      "Epoch 38/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.8854 - acc: 0.4999 - val_loss: 2.3927 - val_acc: 0.3855\n",
      "Epoch 39/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.8589 - acc: 0.5035 - val_loss: 2.3878 - val_acc: 0.3835\n",
      "Epoch 40/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.8311 - acc: 0.5089 - val_loss: 2.3801 - val_acc: 0.3835\n",
      "Epoch 41/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.8184 - acc: 0.5081 - val_loss: 2.3745 - val_acc: 0.3815\n",
      "Epoch 42/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.7754 - acc: 0.5216 - val_loss: 2.3665 - val_acc: 0.3865\n",
      "Epoch 43/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.7729 - acc: 0.5220 - val_loss: 2.3607 - val_acc: 0.3860\n",
      "Epoch 44/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.7387 - acc: 0.5346 - val_loss: 2.3553 - val_acc: 0.3865\n",
      "Epoch 45/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.7219 - acc: 0.5376 - val_loss: 2.3503 - val_acc: 0.3825\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 45, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/45\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 1.7136 - acc: 0.5439 - val_loss: 2.3452 - val_acc: 0.3905\n",
      "Epoch 2/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.6896 - acc: 0.5390 - val_loss: 2.3411 - val_acc: 0.3865\n",
      "Epoch 3/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.6670 - acc: 0.5487 - val_loss: 2.3364 - val_acc: 0.3860\n",
      "Epoch 4/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 1.6531 - acc: 0.5541 - val_loss: 2.3324 - val_acc: 0.3900\n",
      "Epoch 5/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 1.6312 - acc: 0.5646 - val_loss: 2.3274 - val_acc: 0.3855\n",
      "Epoch 6/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.6191 - acc: 0.5619 - val_loss: 2.3241 - val_acc: 0.3860\n",
      "Epoch 7/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.5830 - acc: 0.5705 - val_loss: 2.3205 - val_acc: 0.3890\n",
      "Epoch 8/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.5799 - acc: 0.5701 - val_loss: 2.3147 - val_acc: 0.3925\n",
      "Epoch 9/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.5461 - acc: 0.5803 - val_loss: 2.3124 - val_acc: 0.3910\n",
      "Epoch 10/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.5268 - acc: 0.5876 - val_loss: 2.3072 - val_acc: 0.3870\n",
      "Epoch 11/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.5096 - acc: 0.5889 - val_loss: 2.3046 - val_acc: 0.3850\n",
      "Epoch 12/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 1.4861 - acc: 0.5940 - val_loss: 2.3020 - val_acc: 0.3885\n",
      "Epoch 13/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.4756 - acc: 0.6008 - val_loss: 2.3006 - val_acc: 0.3865\n",
      "Epoch 14/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.4597 - acc: 0.6042 - val_loss: 2.2960 - val_acc: 0.3935\n",
      "Epoch 15/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 1.4359 - acc: 0.6102 - val_loss: 2.2939 - val_acc: 0.3900\n",
      "Epoch 16/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.4224 - acc: 0.6109 - val_loss: 2.2921 - val_acc: 0.3925\n",
      "Epoch 17/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.4186 - acc: 0.6109 - val_loss: 2.2882 - val_acc: 0.3955\n",
      "Epoch 18/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.3851 - acc: 0.6234 - val_loss: 2.2855 - val_acc: 0.3880\n",
      "Epoch 19/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.3661 - acc: 0.6350 - val_loss: 2.2849 - val_acc: 0.3960\n",
      "Epoch 20/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 1.3500 - acc: 0.6351 - val_loss: 2.2807 - val_acc: 0.3905\n",
      "Epoch 21/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.3392 - acc: 0.6375 - val_loss: 2.2804 - val_acc: 0.3950\n",
      "Epoch 22/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.3191 - acc: 0.6482 - val_loss: 2.2796 - val_acc: 0.3895\n",
      "Epoch 23/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.3055 - acc: 0.6436 - val_loss: 2.2766 - val_acc: 0.3895\n",
      "Epoch 24/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.2855 - acc: 0.6538 - val_loss: 2.2761 - val_acc: 0.3950\n",
      "Epoch 25/45\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 1.2772 - acc: 0.6538 - val_loss: 2.2759 - val_acc: 0.3945\n",
      "Epoch 26/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.2574 - acc: 0.6658 - val_loss: 2.2726 - val_acc: 0.3955\n",
      "Epoch 27/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.2395 - acc: 0.6637 - val_loss: 2.2705 - val_acc: 0.3965\n",
      "Epoch 28/45\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 1.2191 - acc: 0.6686 - val_loss: 2.2700 - val_acc: 0.3950\n",
      "Epoch 29/45\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 1.2060 - acc: 0.6763 - val_loss: 2.2696 - val_acc: 0.3990\n",
      "Epoch 30/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.1989 - acc: 0.6789 - val_loss: 2.2691 - val_acc: 0.3965\n",
      "Epoch 31/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.1709 - acc: 0.6881 - val_loss: 2.2672 - val_acc: 0.3970\n",
      "Epoch 32/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.1591 - acc: 0.6889 - val_loss: 2.2653 - val_acc: 0.3960\n",
      "Epoch 33/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 1.1488 - acc: 0.6911 - val_loss: 2.2642 - val_acc: 0.3945\n",
      "Epoch 34/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.1219 - acc: 0.6960 - val_loss: 2.2658 - val_acc: 0.4030\n",
      "Epoch 35/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.1210 - acc: 0.7019 - val_loss: 2.2642 - val_acc: 0.4005\n",
      "Epoch 36/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.1026 - acc: 0.7031 - val_loss: 2.2647 - val_acc: 0.4000\n",
      "Epoch 37/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.0886 - acc: 0.7129 - val_loss: 2.2659 - val_acc: 0.3995\n",
      "Epoch 38/45\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 1.0787 - acc: 0.7116 - val_loss: 2.2640 - val_acc: 0.3945\n",
      "Epoch 39/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 1.0598 - acc: 0.7220 - val_loss: 2.2650 - val_acc: 0.3995\n",
      "Epoch 40/45\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 1.0446 - acc: 0.7231 - val_loss: 2.2613 - val_acc: 0.3985\n",
      "Epoch 41/45\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 1.0251 - acc: 0.7313 - val_loss: 2.2635 - val_acc: 0.3955\n",
      "Epoch 42/45\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 1.0159 - acc: 0.7318 - val_loss: 2.2644 - val_acc: 0.3975\n",
      "Epoch 43/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 0.9959 - acc: 0.7414 - val_loss: 2.2668 - val_acc: 0.4015\n",
      "Epoch 44/45\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 0.9919 - acc: 0.7411 - val_loss: 2.2681 - val_acc: 0.4020\n",
      "Epoch 45/45\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 0.9666 - acc: 0.7428 - val_loss: 2.2662 - val_acc: 0.4020\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 45, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 138us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2661995372772217, 0.402]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Mod√®le basique: LSTM\n",
    "## Embedding non pr√©entrain√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 93)                11997     \n",
      "=================================================================\n",
      "Total params: 1,588,061\n",
      "Trainable params: 1,588,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er un r√©seau √† base de LSTM avec au minimum:\n",
    "# Embedding\n",
    "# Dropout\n",
    "# LSTM\n",
    "# Dropout\n",
    "# Classifieur\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(10000, embed_dim,input_length = 40))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_out)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab,activation='softmax'))\n",
    "\n",
    "# Compiler le mod√®le \n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "# Afficher le summary du mod√®le\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 6s 746us/step - loss: 3.7560 - acc: 0.1670 - val_loss: 3.4126 - val_acc: 0.2315\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 5s 565us/step - loss: 3.2691 - acc: 0.2446 - val_loss: 3.1396 - val_acc: 0.2560\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 4s 562us/step - loss: 2.9673 - acc: 0.2836 - val_loss: 2.9465 - val_acc: 0.3000\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 4s 562us/step - loss: 2.6344 - acc: 0.3385 - val_loss: 2.8653 - val_acc: 0.3275\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 4s 561us/step - loss: 2.2948 - acc: 0.4183 - val_loss: 2.8664 - val_acc: 0.3265\n"
     ]
    }
   ],
   "source": [
    "# Fitter le mod√®le \n",
    "\n",
    "history = model.fit(x_train_seq, y_train_id, batch_size = batch_size, epochs = 5, validation_data=(x_test_seq, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 2s 801us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.866437623977661, 0.3265]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Mod√®le basique: LSTM\n",
    "## Embedding Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 93)                11997     \n",
      "=================================================================\n",
      "Total params: 20,228,239\n",
      "Trainable params: 20,228,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lstm_out = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 40,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_out)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab,activation='softmax'))\n",
    "\n",
    "# Compiler le mod√®le \n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "# Afficher le summary du mod√®le\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/18\n",
      "8000/8000 [==============================] - 7s 888us/step - loss: 3.7192 - acc: 0.1717 - val_loss: 3.3189 - val_acc: 0.2345\n",
      "Epoch 2/18\n",
      "8000/8000 [==============================] - 5s 615us/step - loss: 3.2145 - acc: 0.2476 - val_loss: 2.9274 - val_acc: 0.2880\n",
      "Epoch 3/18\n",
      "8000/8000 [==============================] - 5s 620us/step - loss: 2.9693 - acc: 0.2834 - val_loss: 2.7681 - val_acc: 0.3140\n",
      "Epoch 4/18\n",
      "8000/8000 [==============================] - 5s 614us/step - loss: 2.8094 - acc: 0.3040 - val_loss: 2.6647 - val_acc: 0.3315\n",
      "Epoch 5/18\n",
      "8000/8000 [==============================] - 5s 617us/step - loss: 2.6754 - acc: 0.3250 - val_loss: 2.5819 - val_acc: 0.3505\n",
      "Epoch 6/18\n",
      "8000/8000 [==============================] - 5s 616us/step - loss: 2.5728 - acc: 0.3450 - val_loss: 2.5269 - val_acc: 0.3450\n",
      "Epoch 7/18\n",
      "8000/8000 [==============================] - 5s 620us/step - loss: 2.4876 - acc: 0.3568 - val_loss: 2.4924 - val_acc: 0.3520\n",
      "Epoch 8/18\n",
      "8000/8000 [==============================] - 5s 615us/step - loss: 2.4192 - acc: 0.3710 - val_loss: 2.4452 - val_acc: 0.3600\n",
      "Epoch 9/18\n",
      "8000/8000 [==============================] - 5s 618us/step - loss: 2.3312 - acc: 0.3931 - val_loss: 2.4047 - val_acc: 0.3715\n",
      "Epoch 10/18\n",
      "8000/8000 [==============================] - 5s 615us/step - loss: 2.2790 - acc: 0.4020 - val_loss: 2.3862 - val_acc: 0.3830\n",
      "Epoch 11/18\n",
      "8000/8000 [==============================] - 5s 635us/step - loss: 2.2144 - acc: 0.4161 - val_loss: 2.3782 - val_acc: 0.3940\n",
      "Epoch 12/18\n",
      "8000/8000 [==============================] - 5s 617us/step - loss: 2.1511 - acc: 0.4303 - val_loss: 2.3818 - val_acc: 0.3825\n",
      "Epoch 13/18\n",
      "8000/8000 [==============================] - 5s 619us/step - loss: 2.1088 - acc: 0.4390 - val_loss: 2.3642 - val_acc: 0.3855\n",
      "Epoch 14/18\n",
      "8000/8000 [==============================] - 5s 615us/step - loss: 2.0511 - acc: 0.4466 - val_loss: 2.3480 - val_acc: 0.3900\n",
      "Epoch 15/18\n",
      "8000/8000 [==============================] - 5s 617us/step - loss: 1.9987 - acc: 0.4579 - val_loss: 2.3404 - val_acc: 0.3870\n",
      "Epoch 16/18\n",
      "8000/8000 [==============================] - 5s 616us/step - loss: 1.9490 - acc: 0.4711 - val_loss: 2.3179 - val_acc: 0.3920\n",
      "Epoch 17/18\n",
      "8000/8000 [==============================] - 5s 621us/step - loss: 1.9078 - acc: 0.4733 - val_loss: 2.3385 - val_acc: 0.3975\n",
      "Epoch 18/18\n",
      "8000/8000 [==============================] - 5s 616us/step - loss: 1.8399 - acc: 0.4889 - val_loss: 2.3405 - val_acc: 0.3990\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 18, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 2s 775us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.340456693649292, 0.399]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le mod√®le\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Mod√®le basique: GRU\n",
    "## Embedding non pr√©entrain√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn_model():\n",
    "    embedding_dim = 300\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=40\n",
    "    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    x = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, \n",
    "                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = Adam(lr=1e-4)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer= adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_simple_model = get_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/150\n",
      "8000/8000 [==============================] - 5s 652us/step - loss: 3.9082 - acc: 0.1584 - val_loss: 3.5130 - val_acc: 0.1880\n",
      "Epoch 2/150\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 3.5118 - acc: 0.1878 - val_loss: 3.4665 - val_acc: 0.1880\n",
      "Epoch 3/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 3.4816 - acc: 0.1884 - val_loss: 3.4550 - val_acc: 0.1880\n",
      "Epoch 4/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 3.4711 - acc: 0.1889 - val_loss: 3.4519 - val_acc: 0.1880\n",
      "Epoch 5/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 3.4593 - acc: 0.1885 - val_loss: 3.4439 - val_acc: 0.1880\n",
      "Epoch 6/150\n",
      "8000/8000 [==============================] - 3s 411us/step - loss: 3.4560 - acc: 0.1890 - val_loss: 3.4391 - val_acc: 0.1880\n",
      "Epoch 7/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 3.4547 - acc: 0.1898 - val_loss: 3.4328 - val_acc: 0.1880\n",
      "Epoch 8/150\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 3.4392 - acc: 0.1901 - val_loss: 3.4218 - val_acc: 0.1885\n",
      "Epoch 9/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 3.4307 - acc: 0.1921 - val_loss: 3.4146 - val_acc: 0.1885\n",
      "Epoch 10/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 3.4177 - acc: 0.1958 - val_loss: 3.4042 - val_acc: 0.1925\n",
      "Epoch 11/150\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 3.4070 - acc: 0.1985 - val_loss: 3.3862 - val_acc: 0.2050\n",
      "Epoch 12/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.3919 - acc: 0.2082 - val_loss: 3.3763 - val_acc: 0.2070\n",
      "Epoch 13/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.3809 - acc: 0.2165 - val_loss: 3.3615 - val_acc: 0.2205\n",
      "Epoch 14/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 3.3614 - acc: 0.2233 - val_loss: 3.3456 - val_acc: 0.2225\n",
      "Epoch 15/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 3.3472 - acc: 0.2280 - val_loss: 3.3247 - val_acc: 0.2270\n",
      "Epoch 16/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.3264 - acc: 0.2315 - val_loss: 3.3056 - val_acc: 0.2300\n",
      "Epoch 17/150\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 3.3043 - acc: 0.2338 - val_loss: 3.2903 - val_acc: 0.2335\n",
      "Epoch 18/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.2817 - acc: 0.2370 - val_loss: 3.2739 - val_acc: 0.2450\n",
      "Epoch 19/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.2624 - acc: 0.2439 - val_loss: 3.2560 - val_acc: 0.2475\n",
      "Epoch 20/150\n",
      "8000/8000 [==============================] - 3s 425us/step - loss: 3.2489 - acc: 0.2421 - val_loss: 3.2398 - val_acc: 0.2500\n",
      "Epoch 21/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.2342 - acc: 0.2469 - val_loss: 3.2294 - val_acc: 0.2490\n",
      "Epoch 22/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.2117 - acc: 0.2481 - val_loss: 3.2109 - val_acc: 0.2480\n",
      "Epoch 23/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 3.1978 - acc: 0.2515 - val_loss: 3.1997 - val_acc: 0.2515\n",
      "Epoch 24/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 3.1839 - acc: 0.2526 - val_loss: 3.1851 - val_acc: 0.2525\n",
      "Epoch 25/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.1659 - acc: 0.2571 - val_loss: 3.1731 - val_acc: 0.2560\n",
      "Epoch 26/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 3.1540 - acc: 0.2605 - val_loss: 3.1585 - val_acc: 0.2580\n",
      "Epoch 27/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 3.1324 - acc: 0.2629 - val_loss: 3.1450 - val_acc: 0.2600\n",
      "Epoch 28/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 3.1160 - acc: 0.2635 - val_loss: 3.1293 - val_acc: 0.2665\n",
      "Epoch 29/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 3.1006 - acc: 0.2690 - val_loss: 3.1211 - val_acc: 0.2635\n",
      "Epoch 30/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 3.0867 - acc: 0.2739 - val_loss: 3.1073 - val_acc: 0.2640\n",
      "Epoch 31/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 3.0670 - acc: 0.2740 - val_loss: 3.0958 - val_acc: 0.2665\n",
      "Epoch 32/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 3.0528 - acc: 0.2787 - val_loss: 3.0799 - val_acc: 0.2725\n",
      "Epoch 33/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 3.0341 - acc: 0.2795 - val_loss: 3.0696 - val_acc: 0.2730\n",
      "Epoch 34/150\n",
      "8000/8000 [==============================] - 3s 411us/step - loss: 3.0193 - acc: 0.2860 - val_loss: 3.0601 - val_acc: 0.2825\n",
      "Epoch 35/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 3.0035 - acc: 0.2895 - val_loss: 3.0452 - val_acc: 0.2870\n",
      "Epoch 36/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.9940 - acc: 0.2916 - val_loss: 3.0311 - val_acc: 0.2935\n",
      "Epoch 37/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.9726 - acc: 0.2933 - val_loss: 3.0181 - val_acc: 0.2955\n",
      "Epoch 38/150\n",
      "8000/8000 [==============================] - 3s 423us/step - loss: 2.9505 - acc: 0.2999 - val_loss: 3.0085 - val_acc: 0.2865\n",
      "Epoch 39/150\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 2.9398 - acc: 0.3008 - val_loss: 2.9982 - val_acc: 0.2925\n",
      "Epoch 40/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.9119 - acc: 0.3086 - val_loss: 2.9848 - val_acc: 0.3070\n",
      "Epoch 41/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.8950 - acc: 0.3083 - val_loss: 2.9711 - val_acc: 0.3050\n",
      "Epoch 42/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.8811 - acc: 0.3135 - val_loss: 2.9586 - val_acc: 0.3050\n",
      "Epoch 43/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.8695 - acc: 0.3143 - val_loss: 2.9480 - val_acc: 0.3120\n",
      "Epoch 44/150\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 2.8486 - acc: 0.3199 - val_loss: 2.9369 - val_acc: 0.3060\n",
      "Epoch 45/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.8368 - acc: 0.3235 - val_loss: 2.9253 - val_acc: 0.3160\n",
      "Epoch 46/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.8136 - acc: 0.3265 - val_loss: 2.9183 - val_acc: 0.3115\n",
      "Epoch 47/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.8019 - acc: 0.3287 - val_loss: 2.9111 - val_acc: 0.3115\n",
      "Epoch 48/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.7870 - acc: 0.3342 - val_loss: 2.8953 - val_acc: 0.3170\n",
      "Epoch 49/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.7708 - acc: 0.3407 - val_loss: 2.8856 - val_acc: 0.3135\n",
      "Epoch 50/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.7491 - acc: 0.3391 - val_loss: 2.8722 - val_acc: 0.3210\n",
      "Epoch 51/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.7391 - acc: 0.3422 - val_loss: 2.8657 - val_acc: 0.3240\n",
      "Epoch 52/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.7171 - acc: 0.3480 - val_loss: 2.8572 - val_acc: 0.3190\n",
      "Epoch 53/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.7054 - acc: 0.3484 - val_loss: 2.8463 - val_acc: 0.3285\n",
      "Epoch 54/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.6864 - acc: 0.3554 - val_loss: 2.8384 - val_acc: 0.3280\n",
      "Epoch 55/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.6760 - acc: 0.3534 - val_loss: 2.8291 - val_acc: 0.3320\n",
      "Epoch 56/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.6570 - acc: 0.3596 - val_loss: 2.8270 - val_acc: 0.3265\n",
      "Epoch 57/150\n",
      "8000/8000 [==============================] - 3s 427us/step - loss: 2.6416 - acc: 0.3608 - val_loss: 2.8106 - val_acc: 0.3310\n",
      "Epoch 58/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.6326 - acc: 0.3653 - val_loss: 2.8024 - val_acc: 0.3360\n",
      "Epoch 59/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.6128 - acc: 0.3699 - val_loss: 2.7981 - val_acc: 0.3435\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 3s 412us/step - loss: 2.5991 - acc: 0.3709 - val_loss: 2.7874 - val_acc: 0.3375\n",
      "Epoch 61/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.5809 - acc: 0.3755 - val_loss: 2.7840 - val_acc: 0.3390\n",
      "Epoch 62/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.5666 - acc: 0.3777 - val_loss: 2.7730 - val_acc: 0.3400\n",
      "Epoch 63/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.5468 - acc: 0.3814 - val_loss: 2.7626 - val_acc: 0.3475\n",
      "Epoch 64/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.5322 - acc: 0.3865 - val_loss: 2.7555 - val_acc: 0.3510\n",
      "Epoch 65/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 2.5249 - acc: 0.3890 - val_loss: 2.7516 - val_acc: 0.3485\n",
      "Epoch 66/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.5109 - acc: 0.3891 - val_loss: 2.7427 - val_acc: 0.3520\n",
      "Epoch 67/150\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 2.4872 - acc: 0.3996 - val_loss: 2.7403 - val_acc: 0.3485\n",
      "Epoch 68/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.4831 - acc: 0.3987 - val_loss: 2.7356 - val_acc: 0.3495\n",
      "Epoch 69/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.4639 - acc: 0.3984 - val_loss: 2.7280 - val_acc: 0.3500\n",
      "Epoch 70/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.4499 - acc: 0.4047 - val_loss: 2.7207 - val_acc: 0.3580\n",
      "Epoch 71/150\n",
      "8000/8000 [==============================] - 3s 424us/step - loss: 2.4380 - acc: 0.4094 - val_loss: 2.7106 - val_acc: 0.3565\n",
      "Epoch 72/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.4206 - acc: 0.4078 - val_loss: 2.7095 - val_acc: 0.3565\n",
      "Epoch 73/150\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 2.4074 - acc: 0.4156 - val_loss: 2.6998 - val_acc: 0.3610\n",
      "Epoch 74/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.3886 - acc: 0.4136 - val_loss: 2.6986 - val_acc: 0.3570\n",
      "Epoch 75/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.3747 - acc: 0.4170 - val_loss: 2.6932 - val_acc: 0.3600\n",
      "Epoch 76/150\n",
      "8000/8000 [==============================] - 3s 421us/step - loss: 2.3576 - acc: 0.4195 - val_loss: 2.6880 - val_acc: 0.3615\n",
      "Epoch 77/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.3460 - acc: 0.4282 - val_loss: 2.6798 - val_acc: 0.3635\n",
      "Epoch 78/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.3362 - acc: 0.4288 - val_loss: 2.6803 - val_acc: 0.3575\n",
      "Epoch 79/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.3193 - acc: 0.4348 - val_loss: 2.6710 - val_acc: 0.3600\n",
      "Epoch 80/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.3092 - acc: 0.4335 - val_loss: 2.6695 - val_acc: 0.3655\n",
      "Epoch 81/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.3011 - acc: 0.4363 - val_loss: 2.6648 - val_acc: 0.3650\n",
      "Epoch 82/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 2.2710 - acc: 0.4438 - val_loss: 2.6547 - val_acc: 0.3670\n",
      "Epoch 83/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.2614 - acc: 0.4449 - val_loss: 2.6549 - val_acc: 0.3640\n",
      "Epoch 84/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.2562 - acc: 0.4409 - val_loss: 2.6597 - val_acc: 0.3610\n",
      "Epoch 85/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.2368 - acc: 0.4465 - val_loss: 2.6479 - val_acc: 0.3665\n",
      "Epoch 86/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.2304 - acc: 0.4480 - val_loss: 2.6423 - val_acc: 0.3645\n",
      "Epoch 87/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 2.2074 - acc: 0.4526 - val_loss: 2.6388 - val_acc: 0.3630\n",
      "Epoch 88/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 2.2064 - acc: 0.4549 - val_loss: 2.6354 - val_acc: 0.3645\n",
      "Epoch 89/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.1852 - acc: 0.4599 - val_loss: 2.6296 - val_acc: 0.3670\n",
      "Epoch 90/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 2.1737 - acc: 0.4567 - val_loss: 2.6260 - val_acc: 0.3685\n",
      "Epoch 91/150\n",
      "8000/8000 [==============================] - 3s 411us/step - loss: 2.1460 - acc: 0.4711 - val_loss: 2.6244 - val_acc: 0.3685\n",
      "Epoch 92/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.1437 - acc: 0.4654 - val_loss: 2.6155 - val_acc: 0.3670\n",
      "Epoch 93/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.1313 - acc: 0.4726 - val_loss: 2.6256 - val_acc: 0.3660\n",
      "Epoch 94/150\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 2.1166 - acc: 0.4729 - val_loss: 2.6178 - val_acc: 0.3660\n",
      "Epoch 95/150\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 2.1037 - acc: 0.4791 - val_loss: 2.6159 - val_acc: 0.3685\n",
      "Epoch 96/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.0940 - acc: 0.4808 - val_loss: 2.6083 - val_acc: 0.3670\n",
      "Epoch 97/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.0911 - acc: 0.4796 - val_loss: 2.6067 - val_acc: 0.3700\n",
      "Epoch 98/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.0652 - acc: 0.4865 - val_loss: 2.6012 - val_acc: 0.3720\n",
      "Epoch 99/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 2.0466 - acc: 0.4900 - val_loss: 2.5977 - val_acc: 0.3755\n",
      "Epoch 100/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 2.0289 - acc: 0.4960 - val_loss: 2.5979 - val_acc: 0.3740\n",
      "Epoch 101/150\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 2.0268 - acc: 0.4965 - val_loss: 2.5994 - val_acc: 0.3715\n",
      "Epoch 102/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 2.0061 - acc: 0.4975 - val_loss: 2.6033 - val_acc: 0.3695\n",
      "Epoch 103/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 1.9977 - acc: 0.5025 - val_loss: 2.5966 - val_acc: 0.3685\n",
      "Epoch 104/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 1.9824 - acc: 0.5036 - val_loss: 2.5857 - val_acc: 0.3765\n",
      "Epoch 105/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.9634 - acc: 0.5111 - val_loss: 2.5872 - val_acc: 0.3755\n",
      "Epoch 106/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.9559 - acc: 0.5166 - val_loss: 2.5832 - val_acc: 0.3760\n",
      "Epoch 107/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.9457 - acc: 0.5131 - val_loss: 2.5796 - val_acc: 0.3775\n",
      "Epoch 108/150\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 1.9254 - acc: 0.5214 - val_loss: 2.5814 - val_acc: 0.3725\n",
      "Epoch 109/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.9071 - acc: 0.5232 - val_loss: 2.5799 - val_acc: 0.3740\n",
      "Epoch 110/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 1.9025 - acc: 0.5249 - val_loss: 2.5770 - val_acc: 0.3775\n",
      "Epoch 111/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 1.8875 - acc: 0.5265 - val_loss: 2.5796 - val_acc: 0.3750\n",
      "Epoch 112/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.8756 - acc: 0.5320 - val_loss: 2.5776 - val_acc: 0.3750\n",
      "Epoch 113/150\n",
      "8000/8000 [==============================] - 3s 425us/step - loss: 1.8623 - acc: 0.5327 - val_loss: 2.5746 - val_acc: 0.3755\n",
      "Epoch 114/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.8531 - acc: 0.5356 - val_loss: 2.5700 - val_acc: 0.3740\n",
      "Epoch 115/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 1.8339 - acc: 0.5424 - val_loss: 2.5716 - val_acc: 0.3720\n",
      "Epoch 116/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.8241 - acc: 0.5423 - val_loss: 2.5720 - val_acc: 0.3720\n",
      "Epoch 117/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.8135 - acc: 0.5443 - val_loss: 2.5682 - val_acc: 0.3735\n",
      "Epoch 118/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.8004 - acc: 0.5496 - val_loss: 2.5705 - val_acc: 0.3700\n",
      "Epoch 119/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.7778 - acc: 0.5596 - val_loss: 2.5686 - val_acc: 0.3710\n",
      "Epoch 120/150\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 1.7658 - acc: 0.5611 - val_loss: 2.5633 - val_acc: 0.3740\n",
      "Epoch 121/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 1.7604 - acc: 0.5632 - val_loss: 2.5720 - val_acc: 0.3775\n",
      "Epoch 122/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 1.7495 - acc: 0.5626 - val_loss: 2.5741 - val_acc: 0.3745\n",
      "Epoch 123/150\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 1.7421 - acc: 0.5610 - val_loss: 2.5658 - val_acc: 0.3795\n",
      "Epoch 124/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.7220 - acc: 0.5700 - val_loss: 2.5647 - val_acc: 0.3750\n",
      "Epoch 125/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.7163 - acc: 0.5700 - val_loss: 2.5634 - val_acc: 0.3735\n",
      "Epoch 126/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 1.6977 - acc: 0.5765 - val_loss: 2.5605 - val_acc: 0.3790\n",
      "Epoch 127/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 1.6925 - acc: 0.5824 - val_loss: 2.5642 - val_acc: 0.3750\n",
      "Epoch 128/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.6794 - acc: 0.5771 - val_loss: 2.5631 - val_acc: 0.3780\n",
      "Epoch 129/150\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 1.6603 - acc: 0.5827 - val_loss: 2.5619 - val_acc: 0.3775\n",
      "Epoch 130/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.6510 - acc: 0.5833 - val_loss: 2.5673 - val_acc: 0.3720\n",
      "Epoch 131/150\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 1.6312 - acc: 0.5945 - val_loss: 2.5623 - val_acc: 0.3770\n",
      "Epoch 132/150\n",
      "8000/8000 [==============================] - 3s 422us/step - loss: 1.6220 - acc: 0.5927 - val_loss: 2.5607 - val_acc: 0.3780\n",
      "Epoch 133/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.6078 - acc: 0.5929 - val_loss: 2.5666 - val_acc: 0.3755\n",
      "Epoch 134/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 1.6014 - acc: 0.5992 - val_loss: 2.5713 - val_acc: 0.3765\n",
      "Epoch 135/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 1.5885 - acc: 0.6042 - val_loss: 2.5587 - val_acc: 0.3845\n",
      "Epoch 136/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.5815 - acc: 0.6028 - val_loss: 2.5680 - val_acc: 0.3790\n",
      "Epoch 137/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 1.5656 - acc: 0.6109 - val_loss: 2.5627 - val_acc: 0.3810\n",
      "Epoch 138/150\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 1.5488 - acc: 0.6168 - val_loss: 2.5627 - val_acc: 0.3820\n",
      "Epoch 139/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.5366 - acc: 0.6166 - val_loss: 2.5629 - val_acc: 0.3795\n",
      "Epoch 140/150\n",
      "8000/8000 [==============================] - 3s 411us/step - loss: 1.5249 - acc: 0.6211 - val_loss: 2.5713 - val_acc: 0.3815\n",
      "Epoch 141/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.5227 - acc: 0.6191 - val_loss: 2.5735 - val_acc: 0.3775\n",
      "Epoch 142/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.5059 - acc: 0.6261 - val_loss: 2.5668 - val_acc: 0.3860\n",
      "Epoch 143/150\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 1.4964 - acc: 0.6278 - val_loss: 2.5715 - val_acc: 0.3800\n",
      "Epoch 144/150\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 1.4794 - acc: 0.6275 - val_loss: 2.5710 - val_acc: 0.3855\n",
      "Epoch 145/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.4627 - acc: 0.6310 - val_loss: 2.5703 - val_acc: 0.3800\n",
      "Epoch 146/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.4609 - acc: 0.6386 - val_loss: 2.5702 - val_acc: 0.3895\n",
      "Epoch 147/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.4476 - acc: 0.6404 - val_loss: 2.5702 - val_acc: 0.3865\n",
      "Epoch 148/150\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 1.4405 - acc: 0.6418 - val_loss: 2.5786 - val_acc: 0.3810\n",
      "Epoch 149/150\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 1.4324 - acc: 0.6409 - val_loss: 2.5735 - val_acc: 0.3820\n",
      "Epoch 150/150\n",
      "8000/8000 [==============================] - 3s 422us/step - loss: 1.4087 - acc: 0.6491 - val_loss: 2.5801 - val_acc: 0.3800\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 150\n",
    "history = rnn_simple_model .fit(x=x_train_seq, \n",
    "                    y=y_train_id, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_test_seq, y_test_id),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 546us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.580115997314453, 0.38]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_simple_model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Mod√®le basique: GRU\n",
    "## Embedding Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn_model():\n",
    "    embedding_dim = 50\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=40\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 40,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = Adam(lr=1e-4)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer= adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/150\n",
      "8000/8000 [==============================] - 6s 747us/step - loss: 4.2791 - acc: 0.0546 - val_loss: 3.8614 - val_acc: 0.1860\n",
      "Epoch 2/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 3.6758 - acc: 0.1842 - val_loss: 3.5093 - val_acc: 0.1880\n",
      "Epoch 3/150\n",
      "8000/8000 [==============================] - 4s 468us/step - loss: 3.4969 - acc: 0.1885 - val_loss: 3.4375 - val_acc: 0.1880\n",
      "Epoch 4/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 3.4434 - acc: 0.1891 - val_loss: 3.4048 - val_acc: 0.1880\n",
      "Epoch 5/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 3.4110 - acc: 0.1913 - val_loss: 3.3774 - val_acc: 0.1885\n",
      "Epoch 6/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 3.3834 - acc: 0.2003 - val_loss: 3.3520 - val_acc: 0.1940\n",
      "Epoch 7/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 3.3538 - acc: 0.2082 - val_loss: 3.3215 - val_acc: 0.2105\n",
      "Epoch 8/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 3.3255 - acc: 0.2228 - val_loss: 3.2895 - val_acc: 0.2285\n",
      "Epoch 9/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 3.2942 - acc: 0.2326 - val_loss: 3.2539 - val_acc: 0.2425\n",
      "Epoch 10/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 3.2549 - acc: 0.2452 - val_loss: 3.2168 - val_acc: 0.2475\n",
      "Epoch 11/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 3.2172 - acc: 0.2452 - val_loss: 3.1795 - val_acc: 0.2565\n",
      "Epoch 12/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 3.1846 - acc: 0.2518 - val_loss: 3.1432 - val_acc: 0.2605\n",
      "Epoch 13/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 3.1476 - acc: 0.2609 - val_loss: 3.1078 - val_acc: 0.2720\n",
      "Epoch 14/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 3.1100 - acc: 0.2638 - val_loss: 3.0736 - val_acc: 0.2800\n",
      "Epoch 15/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 3.0777 - acc: 0.2681 - val_loss: 3.0391 - val_acc: 0.2800\n",
      "Epoch 16/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 3.0460 - acc: 0.2690 - val_loss: 3.0109 - val_acc: 0.2850\n",
      "Epoch 17/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 3.0164 - acc: 0.2755 - val_loss: 2.9783 - val_acc: 0.2890\n",
      "Epoch 18/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.9864 - acc: 0.2744 - val_loss: 2.9510 - val_acc: 0.2920\n",
      "Epoch 19/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.9604 - acc: 0.2835 - val_loss: 2.9228 - val_acc: 0.2900\n",
      "Epoch 20/150\n",
      "8000/8000 [==============================] - 4s 470us/step - loss: 2.9232 - acc: 0.2863 - val_loss: 2.8966 - val_acc: 0.2955\n",
      "Epoch 21/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.9013 - acc: 0.2920 - val_loss: 2.8732 - val_acc: 0.3000\n",
      "Epoch 22/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.8788 - acc: 0.2921 - val_loss: 2.8512 - val_acc: 0.3060\n",
      "Epoch 23/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.8513 - acc: 0.3000 - val_loss: 2.8240 - val_acc: 0.3070\n",
      "Epoch 24/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.8330 - acc: 0.2990 - val_loss: 2.8027 - val_acc: 0.3060\n",
      "Epoch 25/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.8056 - acc: 0.3032 - val_loss: 2.7824 - val_acc: 0.3050\n",
      "Epoch 26/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.7838 - acc: 0.3036 - val_loss: 2.7619 - val_acc: 0.3075\n",
      "Epoch 27/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.7669 - acc: 0.3036 - val_loss: 2.7465 - val_acc: 0.3100\n",
      "Epoch 28/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.7435 - acc: 0.3092 - val_loss: 2.7258 - val_acc: 0.3125\n",
      "Epoch 29/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.7217 - acc: 0.3166 - val_loss: 2.7107 - val_acc: 0.3155\n",
      "Epoch 30/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.7079 - acc: 0.3159 - val_loss: 2.6924 - val_acc: 0.3135\n",
      "Epoch 31/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.6866 - acc: 0.3141 - val_loss: 2.6778 - val_acc: 0.3215\n",
      "Epoch 32/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.6758 - acc: 0.3225 - val_loss: 2.6611 - val_acc: 0.3235\n",
      "Epoch 33/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.6524 - acc: 0.3272 - val_loss: 2.6515 - val_acc: 0.3265\n",
      "Epoch 34/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.6379 - acc: 0.3307 - val_loss: 2.6314 - val_acc: 0.3280\n",
      "Epoch 35/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.6197 - acc: 0.3340 - val_loss: 2.6157 - val_acc: 0.3345\n",
      "Epoch 36/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.5999 - acc: 0.3346 - val_loss: 2.6054 - val_acc: 0.3355\n",
      "Epoch 37/150\n",
      "8000/8000 [==============================] - 4s 467us/step - loss: 2.5880 - acc: 0.3357 - val_loss: 2.5892 - val_acc: 0.3320\n",
      "Epoch 38/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.5705 - acc: 0.3395 - val_loss: 2.5810 - val_acc: 0.3335\n",
      "Epoch 39/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.5616 - acc: 0.3449 - val_loss: 2.5724 - val_acc: 0.3315\n",
      "Epoch 40/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.5414 - acc: 0.3486 - val_loss: 2.5610 - val_acc: 0.3390\n",
      "Epoch 41/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.5306 - acc: 0.3501 - val_loss: 2.5476 - val_acc: 0.3440\n",
      "Epoch 42/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.5246 - acc: 0.3493 - val_loss: 2.5424 - val_acc: 0.3445\n",
      "Epoch 43/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.5079 - acc: 0.3523 - val_loss: 2.5292 - val_acc: 0.3405\n",
      "Epoch 44/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.4961 - acc: 0.3526 - val_loss: 2.5249 - val_acc: 0.3490\n",
      "Epoch 45/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.4879 - acc: 0.3554 - val_loss: 2.5123 - val_acc: 0.3485\n",
      "Epoch 46/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.4706 - acc: 0.3580 - val_loss: 2.5053 - val_acc: 0.3475\n",
      "Epoch 47/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.4565 - acc: 0.3630 - val_loss: 2.4941 - val_acc: 0.3460\n",
      "Epoch 48/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.4587 - acc: 0.3611 - val_loss: 2.4881 - val_acc: 0.3495\n",
      "Epoch 49/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.4335 - acc: 0.3608 - val_loss: 2.4772 - val_acc: 0.3535\n",
      "Epoch 50/150\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 2.4188 - acc: 0.3640 - val_loss: 2.4733 - val_acc: 0.3625\n",
      "Epoch 51/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.4150 - acc: 0.3705 - val_loss: 2.4697 - val_acc: 0.3540\n",
      "Epoch 52/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.3954 - acc: 0.3702 - val_loss: 2.4606 - val_acc: 0.3530\n",
      "Epoch 53/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.3871 - acc: 0.3747 - val_loss: 2.4481 - val_acc: 0.3605\n",
      "Epoch 54/150\n",
      "8000/8000 [==============================] - 4s 473us/step - loss: 2.3836 - acc: 0.3804 - val_loss: 2.4473 - val_acc: 0.3635\n",
      "Epoch 55/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 2.3796 - acc: 0.3706 - val_loss: 2.4418 - val_acc: 0.3570\n",
      "Epoch 56/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 2.3645 - acc: 0.3776 - val_loss: 2.4312 - val_acc: 0.3675\n",
      "Epoch 57/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.3592 - acc: 0.3789 - val_loss: 2.4248 - val_acc: 0.3655\n",
      "Epoch 58/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.3463 - acc: 0.3776 - val_loss: 2.4239 - val_acc: 0.3640\n",
      "Epoch 59/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.3380 - acc: 0.3826 - val_loss: 2.4235 - val_acc: 0.3720\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 4s 456us/step - loss: 2.3208 - acc: 0.3869 - val_loss: 2.4127 - val_acc: 0.3635\n",
      "Epoch 61/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.3192 - acc: 0.3904 - val_loss: 2.4136 - val_acc: 0.3725\n",
      "Epoch 62/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.3028 - acc: 0.3885 - val_loss: 2.4037 - val_acc: 0.3725\n",
      "Epoch 63/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.2983 - acc: 0.3824 - val_loss: 2.4019 - val_acc: 0.3715\n",
      "Epoch 64/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.2951 - acc: 0.3865 - val_loss: 2.3977 - val_acc: 0.3700\n",
      "Epoch 65/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.2806 - acc: 0.3926 - val_loss: 2.3841 - val_acc: 0.3755\n",
      "Epoch 66/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.2764 - acc: 0.3903 - val_loss: 2.3886 - val_acc: 0.3720\n",
      "Epoch 67/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.2591 - acc: 0.3957 - val_loss: 2.3828 - val_acc: 0.3755\n",
      "Epoch 68/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.2533 - acc: 0.3959 - val_loss: 2.3767 - val_acc: 0.3740\n",
      "Epoch 69/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.2465 - acc: 0.3984 - val_loss: 2.3724 - val_acc: 0.3780\n",
      "Epoch 70/150\n",
      "8000/8000 [==============================] - 4s 467us/step - loss: 2.2349 - acc: 0.4002 - val_loss: 2.3698 - val_acc: 0.3700\n",
      "Epoch 71/150\n",
      "8000/8000 [==============================] - 4s 466us/step - loss: 2.2231 - acc: 0.4025 - val_loss: 2.3670 - val_acc: 0.3745\n",
      "Epoch 72/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.2259 - acc: 0.4034 - val_loss: 2.3647 - val_acc: 0.3745\n",
      "Epoch 73/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.2133 - acc: 0.4079 - val_loss: 2.3629 - val_acc: 0.3785\n",
      "Epoch 74/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.2102 - acc: 0.4047 - val_loss: 2.3582 - val_acc: 0.3760\n",
      "Epoch 75/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.1987 - acc: 0.4068 - val_loss: 2.3562 - val_acc: 0.3690\n",
      "Epoch 76/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.1948 - acc: 0.4171 - val_loss: 2.3578 - val_acc: 0.3745\n",
      "Epoch 77/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.1837 - acc: 0.4126 - val_loss: 2.3473 - val_acc: 0.3725\n",
      "Epoch 78/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.1814 - acc: 0.4165 - val_loss: 2.3405 - val_acc: 0.3815\n",
      "Epoch 79/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.1657 - acc: 0.4210 - val_loss: 2.3446 - val_acc: 0.3810\n",
      "Epoch 80/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.1696 - acc: 0.4141 - val_loss: 2.3347 - val_acc: 0.3795\n",
      "Epoch 81/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.1501 - acc: 0.4178 - val_loss: 2.3444 - val_acc: 0.3755\n",
      "Epoch 82/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.1408 - acc: 0.4254 - val_loss: 2.3356 - val_acc: 0.3720\n",
      "Epoch 83/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.1431 - acc: 0.4219 - val_loss: 2.3363 - val_acc: 0.3805\n",
      "Epoch 84/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.1369 - acc: 0.4173 - val_loss: 2.3264 - val_acc: 0.3830\n",
      "Epoch 85/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 2.1238 - acc: 0.4256 - val_loss: 2.3320 - val_acc: 0.3770\n",
      "Epoch 86/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.1069 - acc: 0.4266 - val_loss: 2.3239 - val_acc: 0.3825\n",
      "Epoch 87/150\n",
      "8000/8000 [==============================] - 4s 471us/step - loss: 2.1071 - acc: 0.4278 - val_loss: 2.3232 - val_acc: 0.3820\n",
      "Epoch 88/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.0993 - acc: 0.4321 - val_loss: 2.3183 - val_acc: 0.3850\n",
      "Epoch 89/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.0970 - acc: 0.4345 - val_loss: 2.3161 - val_acc: 0.3815\n",
      "Epoch 90/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.0884 - acc: 0.4340 - val_loss: 2.3144 - val_acc: 0.3885\n",
      "Epoch 91/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 2.0830 - acc: 0.4325 - val_loss: 2.3066 - val_acc: 0.3830\n",
      "Epoch 92/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.0709 - acc: 0.4445 - val_loss: 2.3163 - val_acc: 0.3835\n",
      "Epoch 93/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.0632 - acc: 0.4369 - val_loss: 2.3092 - val_acc: 0.3825\n",
      "Epoch 94/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.0583 - acc: 0.4427 - val_loss: 2.3000 - val_acc: 0.3870\n",
      "Epoch 95/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.0517 - acc: 0.4425 - val_loss: 2.2981 - val_acc: 0.3875\n",
      "Epoch 96/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 2.0487 - acc: 0.4456 - val_loss: 2.3070 - val_acc: 0.3845\n",
      "Epoch 97/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.0426 - acc: 0.4471 - val_loss: 2.3074 - val_acc: 0.3835\n",
      "Epoch 98/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.0393 - acc: 0.4442 - val_loss: 2.2957 - val_acc: 0.3850\n",
      "Epoch 99/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.0286 - acc: 0.4450 - val_loss: 2.3066 - val_acc: 0.3795\n",
      "Epoch 100/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 2.0163 - acc: 0.4549 - val_loss: 2.2881 - val_acc: 0.3845\n",
      "Epoch 101/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 2.0060 - acc: 0.4499 - val_loss: 2.2870 - val_acc: 0.3915\n",
      "Epoch 102/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.0083 - acc: 0.4557 - val_loss: 2.2952 - val_acc: 0.3860\n",
      "Epoch 103/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.0053 - acc: 0.4489 - val_loss: 2.3015 - val_acc: 0.3850\n",
      "Epoch 104/150\n",
      "8000/8000 [==============================] - 4s 465us/step - loss: 1.9963 - acc: 0.4499 - val_loss: 2.2855 - val_acc: 0.3905\n",
      "Epoch 105/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 1.9847 - acc: 0.4609 - val_loss: 2.2888 - val_acc: 0.3925\n",
      "Epoch 106/150\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 1.9817 - acc: 0.4650 - val_loss: 2.2819 - val_acc: 0.3895\n",
      "Epoch 107/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 1.9840 - acc: 0.4601 - val_loss: 2.2828 - val_acc: 0.3865\n",
      "Epoch 108/150\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 1.9639 - acc: 0.4577 - val_loss: 2.2875 - val_acc: 0.3820\n",
      "Epoch 109/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 1.9651 - acc: 0.4605 - val_loss: 2.2840 - val_acc: 0.3850\n",
      "Epoch 110/150\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 1.9602 - acc: 0.4672 - val_loss: 2.2776 - val_acc: 0.3890\n",
      "Epoch 111/150\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 1.9505 - acc: 0.4612 - val_loss: 2.2749 - val_acc: 0.3910\n",
      "Epoch 112/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 1.9484 - acc: 0.4664 - val_loss: 2.2794 - val_acc: 0.3840\n",
      "Epoch 113/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.9325 - acc: 0.4685 - val_loss: 2.2793 - val_acc: 0.3860\n",
      "Epoch 114/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 1.9323 - acc: 0.4677 - val_loss: 2.2745 - val_acc: 0.3900\n",
      "Epoch 115/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 1.9223 - acc: 0.4690 - val_loss: 2.2819 - val_acc: 0.3880\n",
      "Epoch 116/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.9310 - acc: 0.4699 - val_loss: 2.2686 - val_acc: 0.3895\n",
      "Epoch 117/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.9146 - acc: 0.4759 - val_loss: 2.2735 - val_acc: 0.3910\n",
      "Epoch 118/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 1.9143 - acc: 0.4751 - val_loss: 2.2681 - val_acc: 0.3870\n",
      "Epoch 119/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.9025 - acc: 0.4744 - val_loss: 2.2731 - val_acc: 0.3885\n",
      "Epoch 120/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.8876 - acc: 0.4763 - val_loss: 2.2647 - val_acc: 0.3915\n",
      "Epoch 121/150\n",
      "8000/8000 [==============================] - 4s 469us/step - loss: 1.8974 - acc: 0.4740 - val_loss: 2.2733 - val_acc: 0.3890\n",
      "Epoch 122/150\n",
      "8000/8000 [==============================] - 4s 465us/step - loss: 1.8809 - acc: 0.4843 - val_loss: 2.2701 - val_acc: 0.3905\n",
      "Epoch 123/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 1.8739 - acc: 0.4838 - val_loss: 2.2699 - val_acc: 0.3905\n",
      "Epoch 124/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.8711 - acc: 0.4838 - val_loss: 2.2635 - val_acc: 0.3940\n",
      "Epoch 125/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.8617 - acc: 0.4856 - val_loss: 2.2702 - val_acc: 0.3880\n",
      "Epoch 126/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.8618 - acc: 0.4847 - val_loss: 2.2601 - val_acc: 0.3940\n",
      "Epoch 127/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 1.8590 - acc: 0.4883 - val_loss: 2.2661 - val_acc: 0.3910\n",
      "Epoch 128/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.8499 - acc: 0.4849 - val_loss: 2.2742 - val_acc: 0.3890\n",
      "Epoch 129/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.8434 - acc: 0.4885 - val_loss: 2.2730 - val_acc: 0.3880\n",
      "Epoch 130/150\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 1.8453 - acc: 0.4895 - val_loss: 2.2601 - val_acc: 0.3945\n",
      "Epoch 131/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.8364 - acc: 0.4951 - val_loss: 2.2645 - val_acc: 0.3935\n",
      "Epoch 132/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.8134 - acc: 0.4949 - val_loss: 2.2616 - val_acc: 0.3945\n",
      "Epoch 133/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.8178 - acc: 0.5006 - val_loss: 2.2740 - val_acc: 0.3880\n",
      "Epoch 134/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.8108 - acc: 0.4967 - val_loss: 2.2574 - val_acc: 0.3930\n",
      "Epoch 135/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.7997 - acc: 0.5029 - val_loss: 2.2581 - val_acc: 0.3965\n",
      "Epoch 136/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 1.7969 - acc: 0.4982 - val_loss: 2.2619 - val_acc: 0.3910\n",
      "Epoch 137/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 1.7916 - acc: 0.5072 - val_loss: 2.2581 - val_acc: 0.3900\n",
      "Epoch 138/150\n",
      "8000/8000 [==============================] - 4s 470us/step - loss: 1.7831 - acc: 0.5086 - val_loss: 2.2577 - val_acc: 0.3945\n",
      "Epoch 139/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.7902 - acc: 0.5062 - val_loss: 2.2544 - val_acc: 0.3985\n",
      "Epoch 140/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.7824 - acc: 0.5032 - val_loss: 2.2635 - val_acc: 0.3965\n",
      "Epoch 141/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.7718 - acc: 0.5031 - val_loss: 2.2562 - val_acc: 0.3990\n",
      "Epoch 142/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.7669 - acc: 0.5079 - val_loss: 2.2603 - val_acc: 0.3915\n",
      "Epoch 143/150\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 1.7700 - acc: 0.5042 - val_loss: 2.2654 - val_acc: 0.3900\n",
      "Epoch 144/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.7593 - acc: 0.5135 - val_loss: 2.2542 - val_acc: 0.3920\n",
      "Epoch 145/150\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.7553 - acc: 0.5111 - val_loss: 2.2622 - val_acc: 0.3920\n",
      "Epoch 146/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.7467 - acc: 0.5076 - val_loss: 2.2658 - val_acc: 0.3955\n",
      "Epoch 147/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.7409 - acc: 0.5199 - val_loss: 2.2586 - val_acc: 0.3980\n",
      "Epoch 148/150\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 1.7369 - acc: 0.5138 - val_loss: 2.2590 - val_acc: 0.3955\n",
      "Epoch 149/150\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 1.7292 - acc: 0.5198 - val_loss: 2.2604 - val_acc: 0.3905\n",
      "Epoch 150/150\n",
      "8000/8000 [==============================] - 4s 458us/step - loss: 1.7260 - acc: 0.5172 - val_loss: 2.2583 - val_acc: 0.3950\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = 128, epochs = 150, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 573us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2582788410186767, 0.395]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_RNN\n",
    "\n",
    "## Embedding non pr√©entrain√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8000/8000 [==============================] - 6s 722us/step - loss: 3.6181 - acc: 0.1758 - val_loss: 3.4511 - val_acc: 0.1900\n",
      "Epoch 2/15\n",
      "8000/8000 [==============================] - 3s 426us/step - loss: 3.4283 - acc: 0.2122 - val_loss: 3.3014 - val_acc: 0.2505\n",
      "Epoch 3/15\n",
      "8000/8000 [==============================] - 3s 422us/step - loss: 3.2694 - acc: 0.2461 - val_loss: 3.1884 - val_acc: 0.2705\n",
      "Epoch 4/15\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 3.1529 - acc: 0.2654 - val_loss: 3.1170 - val_acc: 0.2910\n",
      "Epoch 5/15\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 3.0234 - acc: 0.2901 - val_loss: 3.0412 - val_acc: 0.3085\n",
      "Epoch 6/15\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 2.9029 - acc: 0.3130 - val_loss: 2.9477 - val_acc: 0.3175\n",
      "Epoch 7/15\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 2.7715 - acc: 0.3371 - val_loss: 2.8694 - val_acc: 0.3325\n",
      "Epoch 8/15\n",
      "8000/8000 [==============================] - 3s 421us/step - loss: 2.6190 - acc: 0.3633 - val_loss: 2.8282 - val_acc: 0.3335\n",
      "Epoch 9/15\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 2.4481 - acc: 0.3950 - val_loss: 2.7531 - val_acc: 0.3420\n",
      "Epoch 10/15\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 2.2911 - acc: 0.4273 - val_loss: 2.7322 - val_acc: 0.3565\n",
      "Epoch 11/15\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 2.1278 - acc: 0.4667 - val_loss: 2.6878 - val_acc: 0.3615\n",
      "Epoch 12/15\n",
      "8000/8000 [==============================] - 3s 420us/step - loss: 1.9407 - acc: 0.5109 - val_loss: 2.7397 - val_acc: 0.3500\n",
      "Epoch 13/15\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 1.8049 - acc: 0.5419 - val_loss: 2.7102 - val_acc: 0.3610\n",
      "Epoch 14/15\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 1.6174 - acc: 0.5929 - val_loss: 2.7466 - val_acc: 0.3595\n",
      "Epoch 15/15\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 1.4444 - acc: 0.6388 - val_loss: 2.8039 - val_acc: 0.3490\n"
     ]
    }
   ],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=40\n",
    "    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))\n",
    "    inp = Input(shape=(40, ))\n",
    "    x = Embedding(MAX_NB_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_rnn_cnn_model()\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "history = model.fit(x=x_train_seq, \n",
    "                    y=y_train_id, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_test_seq, y_test_id),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 610us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.803877523422241, 0.349]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_RNN\n",
    "\n",
    "## Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8000/8000 [==============================] - 7s 813us/step - loss: 3.5522 - acc: 0.1958 - val_loss: 3.1862 - val_acc: 0.2730\n",
      "Epoch 2/15\n",
      "8000/8000 [==============================] - 4s 459us/step - loss: 3.0713 - acc: 0.2751 - val_loss: 2.8666 - val_acc: 0.2995\n",
      "Epoch 3/15\n",
      "8000/8000 [==============================] - 4s 465us/step - loss: 2.8125 - acc: 0.3050 - val_loss: 2.6860 - val_acc: 0.3235\n",
      "Epoch 4/15\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.6535 - acc: 0.3314 - val_loss: 2.5593 - val_acc: 0.3490\n",
      "Epoch 5/15\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 2.5127 - acc: 0.3616 - val_loss: 2.4746 - val_acc: 0.3570\n",
      "Epoch 6/15\n",
      "8000/8000 [==============================] - 4s 469us/step - loss: 2.3971 - acc: 0.3819 - val_loss: 2.4147 - val_acc: 0.3665\n",
      "Epoch 7/15\n",
      "8000/8000 [==============================] - 4s 460us/step - loss: 2.3084 - acc: 0.3939 - val_loss: 2.4008 - val_acc: 0.3635\n",
      "Epoch 8/15\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 2.2151 - acc: 0.4136 - val_loss: 2.3583 - val_acc: 0.3670\n",
      "Epoch 9/15\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 2.1518 - acc: 0.4199 - val_loss: 2.3522 - val_acc: 0.3765\n",
      "Epoch 10/15\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 2.0739 - acc: 0.4410 - val_loss: 2.3375 - val_acc: 0.3875\n",
      "Epoch 11/15\n",
      "8000/8000 [==============================] - 4s 464us/step - loss: 1.9897 - acc: 0.4550 - val_loss: 2.3104 - val_acc: 0.3825\n",
      "Epoch 12/15\n",
      "8000/8000 [==============================] - 4s 464us/step - loss: 1.9108 - acc: 0.4825 - val_loss: 2.3300 - val_acc: 0.3815\n",
      "Epoch 13/15\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.8402 - acc: 0.4899 - val_loss: 2.3248 - val_acc: 0.3900\n",
      "Epoch 14/15\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 1.7611 - acc: 0.5132 - val_loss: 2.3305 - val_acc: 0.3890\n",
      "Epoch 15/15\n",
      "8000/8000 [==============================] - 4s 461us/step - loss: 1.7103 - acc: 0.5194 - val_loss: 2.3369 - val_acc: 0.3885\n"
     ]
    }
   ],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 50\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=40\n",
    "    \n",
    "    inp = Input(shape=(40, ))\n",
    "    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 40,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_rnn_cnn_model()\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "history = model.fit(X_train_glove,\n",
    "                    y_train_id, \n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 549us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3368661251068117, 0.3885]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
