{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate, BatchNormalization\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les Train et test pour les abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (8000,), (2000,), (2000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/abstract_only\"\n",
    "feature = \"abstract\"\n",
    "label = 'label'\n",
    "x_train = pd.read_csv(f\"{data_dir}/train.csv\")[feature]\n",
    "y_train = pd.read_csv(f\"{data_dir}/train.csv\")[label]\n",
    "\n",
    "x_test = pd.read_csv(f\"{data_dir}/test.csv\")[feature]\n",
    "y_test = pd.read_csv(f\"{data_dir}/test.csv\")[label]\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manipulation de y:catégorie: label\n",
    "def create_vocab(dt): \n",
    "  to_id = {'<PAD>': 0, '<UNK>':1}\n",
    "\n",
    "  for sent in dt:\n",
    "    for w in sent: \n",
    "      if w not in to_id.keys():\n",
    "        to_id[w] = len(to_id)\n",
    "\n",
    "  from_id = {v: k for k, v in to_id.items()}\n",
    "\n",
    "  vocab = len(to_id.keys())\n",
    "\n",
    "  return to_id, from_id, vocab\n",
    "\n",
    "def preprocess_Y(Y, cat_to_id): \n",
    "  res = []\n",
    "  for ex in Y: \n",
    "    if ex not in cat_to_id.keys():\n",
    "      res.append(cat_to_id['<UNK>'])\n",
    "    else:\n",
    "      res.append(cat_to_id[ex])\n",
    "  return np.array(res)\n",
    "\n",
    "cat_to_id, cat_from_id, cat_vocab = create_vocab([y_train])\n",
    "y_train_id = preprocess_Y(y_train, cat_to_id)\n",
    "y_test_id = preprocess_Y(y_test, cat_to_id)\n",
    "\n",
    "#Manipulation des titres \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq =  tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_seq = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen = 888, truncating='post')\n",
    "x_test_seq = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen = 888, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#une fonction pour visualiser accuracy et loss\n",
    "def plot_history(history, metric=\"acc\"):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.title(f'model {metric}')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahlem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Fonction permettant de charger un embedding \n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_glove_embeddings(fp, embedding_dim, include_empty_char=True):\n",
    "    \"\"\"\n",
    "    Loads pre-trained word embeddings (GloVe embeddings)\n",
    "        Inputs: - fp: filepath of pre-trained glove embeddings\n",
    "                - embedding_dim: dimension of each vector embedding\n",
    "                - generate_matrix: whether to generate an embedding matrix\n",
    "        Outputs:\n",
    "                - word2coefs: Dictionary. Word to its corresponding coefficients\n",
    "                - word2index: Dictionary. Word to word-index\n",
    "                - embedding_matrix: Embedding matrix for Keras Embedding layer\n",
    "    \"\"\"\n",
    "    # First, build the \"word2coefs\" and \"word2index\"\n",
    "    word2coefs = {} # word to its corresponding coefficients\n",
    "    word2index = {} # word to word-index\n",
    "    with open(fp) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            try:\n",
    "                data = [x.strip().lower() for x in line.split()]\n",
    "                word = data[0]\n",
    "                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')\n",
    "                word2coefs[word] = coefs\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = len(word2index)\n",
    "            except Exception as e:\n",
    "                print('Exception occurred in `load_glove_embeddings`:', e)\n",
    "                continue\n",
    "        # End of for loop.\n",
    "    # End of with open\n",
    "    if include_empty_char:\n",
    "        word2index[''] = len(word2index)\n",
    "    # Second, build the \"embedding_matrix\"\n",
    "    # Words not found in embedding index will be all-zeros. Hence, the \"+1\".\n",
    "    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in word2index.items():\n",
    "        embedding_vec = word2coefs.get(word)\n",
    "        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:\n",
    "            embedding_matrix[idx] = np.asarray(embedding_vec)\n",
    "    # return word2coefs, word2index, embedding_matrix\n",
    "    return word2index, np.asarray(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les embeddings à l'aide de la fonction load_glove_embeddings\n",
    "\n",
    "word2index, embedding_matrix = load_glove_embeddings('glove.6B.50d.txt', embedding_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9247   4415   7693  11231     23   7215     24      6      0   9247\n",
      "   6723     14      7   1013   2741    747    445      4   5998  26431\n",
      "     46  10645  12168      3    933      5  23925  11231   4276     10\n",
      "     37  11231    747     17    333   2741   5306      2   2317      1\n",
      "    197      4   6326      0   9247   4415   7693    747     17    333\n",
      "     23  10645     24     46  21699    933   8931     56    999      6\n",
      "      0   9247    610      2     10   3568      0  11231   9009      3\n",
      "   9247   4415   7693      1     37   1307  10388      7   1999 226671\n",
      "   2741  12597    622      0  12886  24113  26479   1587      4  33062\n",
      "      0  12838      3  21566   4341      3  19847   2817      1     42\n",
      "     14    175      0 201534   2741  12597      2      0  14575    604\n",
      "     17      0   9247   4415   7693  75816    273     12      1     21\n",
      "  54193      0  21566  12838      1      0   1225  12597     31    219\n",
      "  35743    883      5 112878 226671      5      0     68   6223      6\n",
      "   1079      3   7215   9009      2     68   3025    671  14575    604\n",
      "     52    273      0   5324      5   8305      3      0   1225  12597\n",
      "      2]\n",
      "[    53     32   2542      6   6707   1046      6  16631    933     21\n",
      "  13215   2366     29 201534  45209      5   8326      0   5865      3\n",
      "  13416     23   2233     24     74     79      2     53   6071      7\n",
      "    426  41935    933    208     45      0  13070 201534   6351      1\n",
      "      7   2022    849      3  12257   6939   3270      2   6707   1046\n",
      "     74     79   2967      7  45209  12597     12   5080  13416   3328\n",
      "    124    357  68301      3      0   8060    933      2    212      1\n",
      "    357  68301      3      0 201534    933    410      4   1209   1046\n",
      "      4     96      3      0  13416      2     48   1247     10     37\n",
      "     14     12      0    460      6     42   3270    425   2233     32\n",
      "   2078     14   3779  11989      2    212      1   1192  40695      3\n",
      "   3270      1    175   1132   2233      1  17874      4    567   1972\n",
      "      6      0 201534   6351      5   1580   1650      6    130  45209\n",
      "      2     21   8543      0  20845      3  13416     12    944   3328\n",
      "    124   2801  45209    976      1     53    169      0    208      3\n",
      "   1132   2233     12     53     86   1008     74     79      2     53\n",
      "   5928     12    125   1132   2233    949     95      4   3559   3040\n",
      "   2233      5   1008  21208   1046      6      0   6814   1972      3\n",
      "    162    849    933      2]\n"
     ]
    }
   ],
   "source": [
    "# ecrire une fonction de tokenization custom pour preprocesser les textes\n",
    "\n",
    "def custom_tokenize(doc):\n",
    "  res = []\n",
    "  for ex in word_tokenize(doc): \n",
    "    if ex not in word2index.keys():\n",
    "      res.append(word2index['unk'])\n",
    "    else:\n",
    "      res.append(word2index[ex])\n",
    "  return np.array(res)\n",
    "    \n",
    "# Encoder les textes avec la fonction custom\n",
    "X_train_glove = [custom_tokenize(x) for x in x_train]\n",
    "print(X_train_glove[0])\n",
    "\n",
    "X_test_glove = [custom_tokenize(x) for x in x_test]\n",
    "print(X_test_glove[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding des sequences\n",
    "\n",
    "X_train_glove = tf.keras.preprocessing.sequence.pad_sequences(X_train_glove, maxlen = 888, truncating='post')\n",
    "X_test_glove = tf.keras.preprocessing.sequence.pad_sequences(X_test_glove, maxlen = 888, truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Modèle basique\n",
    "## Embedding non préentrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 888, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 887, 32)           8224      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 93)                11997     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 93)                0         \n",
      "=================================================================\n",
      "Total params: 1,304,445\n",
      "Trainable params: 1,304,445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential() # modèle séquentiel\n",
    "model.add(tf.keras.layers.Embedding(10000, embed_dim,input_length = 888))# couche d'embedding de taille 128\n",
    "model.add(tf.keras.layers.Conv1D(32,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Compiler le modèle \n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "# Afficher le summary du modèle\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 3.8483 - acc: 0.1804 - val_loss: 3.3909 - val_acc: 0.1975\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3.2230 - acc: 0.2460 - val_loss: 3.0656 - val_acc: 0.2740\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 2.9124 - acc: 0.3006 - val_loss: 2.8065 - val_acc: 0.3280\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 2.6171 - acc: 0.3593 - val_loss: 2.6141 - val_acc: 0.3600\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 2.3509 - acc: 0.4086 - val_loss: 2.5057 - val_acc: 0.3735\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2.1099 - acc: 0.4556 - val_loss: 2.4455 - val_acc: 0.3950\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1.8561 - acc: 0.5154 - val_loss: 2.4428 - val_acc: 0.3870\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1.6436 - acc: 0.5690 - val_loss: 2.4796 - val_acc: 0.3815\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 1.4167 - acc: 0.6262 - val_loss: 2.5639 - val_acc: 0.3845\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 1.2209 - acc: 0.6760 - val_loss: 2.6589 - val_acc: 0.3800\n"
     ]
    }
   ],
   "source": [
    "# Fitter le modèle \n",
    "\n",
    "history = model.fit(x_train_seq, y_train_id, batch_size = batch_size, epochs = 10, validation_data=(x_test_seq, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualiser\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.65889536857605, 0.38]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le modèle\n",
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Modèle basique\n",
    "## Embedding GloVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 888, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 887, 32)           3232      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 93)                11997     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 93)                0         \n",
      "=================================================================\n",
      "Total params: 20,019,503\n",
      "Trainable params: 20,019,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Entrainer un modèle en chargeant les poids des embeddings dans le layer Embedding\n",
    "\n",
    "#embed_dim = 50\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 888,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True))\n",
    "model.add(tf.keras.layers.Conv1D(32,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=adam, metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 1.5855 - acc: 0.5769 - val_loss: 2.4230 - val_acc: 0.3880\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 1.4830 - acc: 0.6019 - val_loss: 2.4602 - val_acc: 0.3890\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 1.3718 - acc: 0.6302 - val_loss: 2.5146 - val_acc: 0.3835\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 1.2926 - acc: 0.6521 - val_loss: 2.5514 - val_acc: 0.3785\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 1.1854 - acc: 0.6760 - val_loss: 2.5885 - val_acc: 0.3740\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 15, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluer le modèle\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Modèle basique, Changement hyperparamètres\n",
    "## Embedding non préentrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 4.4406 - acc: 0.1318 - val_loss: 4.3187 - val_acc: 0.1880\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 4.1282 - acc: 0.1879 - val_loss: 3.8655 - val_acc: 0.1880\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 3.7127 - acc: 0.1874 - val_loss: 3.5469 - val_acc: 0.1880\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 3.5647 - acc: 0.1846 - val_loss: 3.4735 - val_acc: 0.1880\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 3.5142 - acc: 0.1908 - val_loss: 3.4342 - val_acc: 0.1880\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 3.4634 - acc: 0.2014 - val_loss: 3.3882 - val_acc: 0.2095\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 3.4122 - acc: 0.2223 - val_loss: 3.3301 - val_acc: 0.2375\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 3.3438 - acc: 0.2426 - val_loss: 3.2549 - val_acc: 0.2550\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 3.2693 - acc: 0.2579 - val_loss: 3.1872 - val_acc: 0.2605\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 3.1974 - acc: 0.2729 - val_loss: 3.1301 - val_acc: 0.2805\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 3.1280 - acc: 0.2846 - val_loss: 3.0789 - val_acc: 0.2955\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 3.0709 - acc: 0.2987 - val_loss: 3.0323 - val_acc: 0.3090\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 3.0231 - acc: 0.3092 - val_loss: 2.9884 - val_acc: 0.3135\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 2.9698 - acc: 0.3131 - val_loss: 2.9471 - val_acc: 0.3160\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 2.9215 - acc: 0.3206 - val_loss: 2.9068 - val_acc: 0.3190\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 2.8763 - acc: 0.3284 - val_loss: 2.8684 - val_acc: 0.3225\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 2.8283 - acc: 0.3345 - val_loss: 2.8311 - val_acc: 0.3310\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 2.7854 - acc: 0.3429 - val_loss: 2.7944 - val_acc: 0.3350\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 2.7296 - acc: 0.3541 - val_loss: 2.7584 - val_acc: 0.3340\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 2.6909 - acc: 0.3543 - val_loss: 2.7265 - val_acc: 0.3355\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 2.6602 - acc: 0.3651 - val_loss: 2.6951 - val_acc: 0.3430\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 2.6014 - acc: 0.3730 - val_loss: 2.6657 - val_acc: 0.3485\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 2.5665 - acc: 0.3814 - val_loss: 2.6370 - val_acc: 0.3550\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 2.5307 - acc: 0.3856 - val_loss: 2.6092 - val_acc: 0.3635\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 2.4928 - acc: 0.3930 - val_loss: 2.5835 - val_acc: 0.3680\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 2.4496 - acc: 0.4007 - val_loss: 2.5587 - val_acc: 0.3755\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 2.4170 - acc: 0.4114 - val_loss: 2.5363 - val_acc: 0.3790\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 2.3752 - acc: 0.4184 - val_loss: 2.5149 - val_acc: 0.3835\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 2.3412 - acc: 0.4294 - val_loss: 2.4938 - val_acc: 0.3860\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 2.3127 - acc: 0.4291 - val_loss: 2.4763 - val_acc: 0.3950\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 2.2709 - acc: 0.4408 - val_loss: 2.4568 - val_acc: 0.3975\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 2.2383 - acc: 0.4492 - val_loss: 2.4387 - val_acc: 0.3985\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 2.2000 - acc: 0.4516 - val_loss: 2.4229 - val_acc: 0.4040\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 2.1699 - acc: 0.4654 - val_loss: 2.4077 - val_acc: 0.4105\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 2.1403 - acc: 0.4704 - val_loss: 2.3921 - val_acc: 0.4130\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 2.0974 - acc: 0.4779 - val_loss: 2.3778 - val_acc: 0.4150\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 2.0570 - acc: 0.4910 - val_loss: 2.3648 - val_acc: 0.4180\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 2.0211 - acc: 0.4969 - val_loss: 2.3530 - val_acc: 0.4205\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 1.9982 - acc: 0.4982 - val_loss: 2.3416 - val_acc: 0.4215\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 1.9563 - acc: 0.5151 - val_loss: 2.3302 - val_acc: 0.4240\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 1.9366 - acc: 0.5211 - val_loss: 2.3211 - val_acc: 0.4225\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 1.8930 - acc: 0.5258 - val_loss: 2.3102 - val_acc: 0.4260\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 1.8600 - acc: 0.5389 - val_loss: 2.3004 - val_acc: 0.4260\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 1.8182 - acc: 0.5464 - val_loss: 2.2927 - val_acc: 0.4325\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 1.8028 - acc: 0.5473 - val_loss: 2.2854 - val_acc: 0.4305\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 1.7540 - acc: 0.5640 - val_loss: 2.2776 - val_acc: 0.4305\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 1.7182 - acc: 0.5674 - val_loss: 2.2708 - val_acc: 0.4300\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 1.6929 - acc: 0.5789 - val_loss: 2.2648 - val_acc: 0.4385\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 1.6607 - acc: 0.5866 - val_loss: 2.2614 - val_acc: 0.4370\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 1.6223 - acc: 0.5975 - val_loss: 2.2557 - val_acc: 0.4370\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential() # modèle séquentiel\n",
    "model.add(tf.keras.layers.Embedding(10000, embed_dim,input_length = 888))# couche d'embedding de taille \n",
    "model.add(tf.keras.layers.Conv1D(256,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Compiler le modèle \n",
    "adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=adam ,metrics = ['accuracy'])\n",
    "# Fitter le modèle \n",
    "history = model.fit(x_train_seq, y_train_id, batch_size = batch_size, epochs = 50, validation_data=(x_test_seq, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 98us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2557374572753908, 0.437]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le modèle\n",
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Modèle basique, Changement hyperparamètres\n",
    "## Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/70\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 4.3296 - acc: 0.0999 - val_loss: 3.8339 - val_acc: 0.1880\n",
      "Epoch 2/70\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 3.8327 - acc: 0.1678 - val_loss: 3.5948 - val_acc: 0.1880\n",
      "Epoch 3/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 3.6888 - acc: 0.1771 - val_loss: 3.4987 - val_acc: 0.1880\n",
      "Epoch 4/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 3.6042 - acc: 0.1884 - val_loss: 3.4376 - val_acc: 0.1905\n",
      "Epoch 5/70\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 3.5318 - acc: 0.1964 - val_loss: 3.3829 - val_acc: 0.2030\n",
      "Epoch 6/70\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 3.4791 - acc: 0.2080 - val_loss: 3.3332 - val_acc: 0.2475\n",
      "Epoch 7/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 3.4117 - acc: 0.2243 - val_loss: 3.2774 - val_acc: 0.2620\n",
      "Epoch 8/70\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 3.3247 - acc: 0.2421 - val_loss: 3.2161 - val_acc: 0.2785\n",
      "Epoch 9/70\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 3.2781 - acc: 0.2571 - val_loss: 3.1556 - val_acc: 0.2885\n",
      "Epoch 10/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 3.2002 - acc: 0.2704 - val_loss: 3.0984 - val_acc: 0.2950\n",
      "Epoch 11/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 3.1369 - acc: 0.2805 - val_loss: 3.0390 - val_acc: 0.3040\n",
      "Epoch 12/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 3.0793 - acc: 0.2936 - val_loss: 2.9897 - val_acc: 0.3215\n",
      "Epoch 13/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 3.0227 - acc: 0.2986 - val_loss: 2.9277 - val_acc: 0.3170\n",
      "Epoch 14/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.9558 - acc: 0.3095 - val_loss: 2.8780 - val_acc: 0.3225\n",
      "Epoch 15/70\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 2.9124 - acc: 0.3099 - val_loss: 2.8365 - val_acc: 0.3290\n",
      "Epoch 16/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.8700 - acc: 0.3227 - val_loss: 2.7977 - val_acc: 0.3285\n",
      "Epoch 17/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.8330 - acc: 0.3274 - val_loss: 2.7662 - val_acc: 0.3410\n",
      "Epoch 18/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.7822 - acc: 0.3312 - val_loss: 2.7232 - val_acc: 0.3395\n",
      "Epoch 19/70\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 2.7480 - acc: 0.3377 - val_loss: 2.6929 - val_acc: 0.3380\n",
      "Epoch 20/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 2.7161 - acc: 0.3374 - val_loss: 2.6667 - val_acc: 0.3405\n",
      "Epoch 21/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 2.6779 - acc: 0.3498 - val_loss: 2.6380 - val_acc: 0.3435\n",
      "Epoch 22/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 2.6458 - acc: 0.3486 - val_loss: 2.6161 - val_acc: 0.3470\n",
      "Epoch 23/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 2.6226 - acc: 0.3545 - val_loss: 2.5970 - val_acc: 0.3425\n",
      "Epoch 24/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.5915 - acc: 0.3524 - val_loss: 2.5687 - val_acc: 0.3465\n",
      "Epoch 25/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.5674 - acc: 0.3651 - val_loss: 2.5502 - val_acc: 0.3535\n",
      "Epoch 26/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.5537 - acc: 0.3591 - val_loss: 2.5315 - val_acc: 0.3615\n",
      "Epoch 27/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.5271 - acc: 0.3740 - val_loss: 2.5140 - val_acc: 0.3550\n",
      "Epoch 28/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.4931 - acc: 0.3683 - val_loss: 2.4951 - val_acc: 0.3590\n",
      "Epoch 29/70\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 2.4752 - acc: 0.3756 - val_loss: 2.4795 - val_acc: 0.3620\n",
      "Epoch 30/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.4447 - acc: 0.3817 - val_loss: 2.4612 - val_acc: 0.3635\n",
      "Epoch 31/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.4171 - acc: 0.3910 - val_loss: 2.4463 - val_acc: 0.3685\n",
      "Epoch 32/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.4039 - acc: 0.3846 - val_loss: 2.4301 - val_acc: 0.3755\n",
      "Epoch 33/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.3944 - acc: 0.3877 - val_loss: 2.4187 - val_acc: 0.3765\n",
      "Epoch 34/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 2.3536 - acc: 0.3911 - val_loss: 2.4042 - val_acc: 0.3890\n",
      "Epoch 35/70\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 2.3327 - acc: 0.4024 - val_loss: 2.3870 - val_acc: 0.3820\n",
      "Epoch 36/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.3265 - acc: 0.4020 - val_loss: 2.3807 - val_acc: 0.3815\n",
      "Epoch 37/70\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 2.3045 - acc: 0.4093 - val_loss: 2.3636 - val_acc: 0.3920\n",
      "Epoch 38/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 2.2874 - acc: 0.4124 - val_loss: 2.3535 - val_acc: 0.3930\n",
      "Epoch 39/70\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 2.2552 - acc: 0.4186 - val_loss: 2.3411 - val_acc: 0.4005\n",
      "Epoch 40/70\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 2.2411 - acc: 0.4226 - val_loss: 2.3261 - val_acc: 0.4030\n",
      "Epoch 41/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.2246 - acc: 0.4284 - val_loss: 2.3177 - val_acc: 0.3975\n",
      "Epoch 42/70\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 2.2116 - acc: 0.4286 - val_loss: 2.3030 - val_acc: 0.4060\n",
      "Epoch 43/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.1885 - acc: 0.4370 - val_loss: 2.2929 - val_acc: 0.4030\n",
      "Epoch 44/70\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 2.1753 - acc: 0.4327 - val_loss: 2.2847 - val_acc: 0.4100\n",
      "Epoch 45/70\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 2.1488 - acc: 0.4416 - val_loss: 2.2746 - val_acc: 0.4115\n",
      "Epoch 46/70\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 2.1380 - acc: 0.4453 - val_loss: 2.2618 - val_acc: 0.4135\n",
      "Epoch 47/70\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 2.1235 - acc: 0.4479 - val_loss: 2.2542 - val_acc: 0.4135\n",
      "Epoch 48/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 2.1117 - acc: 0.4506 - val_loss: 2.2484 - val_acc: 0.4100\n",
      "Epoch 49/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 2.0826 - acc: 0.4584 - val_loss: 2.2354 - val_acc: 0.4205\n",
      "Epoch 50/70\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 2.0703 - acc: 0.4549 - val_loss: 2.2325 - val_acc: 0.4175\n",
      "Epoch 51/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.0697 - acc: 0.4586 - val_loss: 2.2213 - val_acc: 0.4160\n",
      "Epoch 52/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.0333 - acc: 0.4624 - val_loss: 2.2118 - val_acc: 0.4185\n",
      "Epoch 53/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 2.0256 - acc: 0.4636 - val_loss: 2.2057 - val_acc: 0.4220\n",
      "Epoch 54/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 2.0096 - acc: 0.4684 - val_loss: 2.2022 - val_acc: 0.4200\n",
      "Epoch 55/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 1.9878 - acc: 0.4783 - val_loss: 2.1927 - val_acc: 0.4235\n",
      "Epoch 56/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 1.9807 - acc: 0.4808 - val_loss: 2.1891 - val_acc: 0.4230\n",
      "Epoch 57/70\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 1.9646 - acc: 0.4811 - val_loss: 2.1747 - val_acc: 0.4300\n",
      "Epoch 58/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 1.9442 - acc: 0.4880 - val_loss: 2.1711 - val_acc: 0.4265\n",
      "Epoch 59/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 1.9283 - acc: 0.4918 - val_loss: 2.1632 - val_acc: 0.4285\n",
      "Epoch 60/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 176us/step - loss: 1.9290 - acc: 0.4918 - val_loss: 2.1574 - val_acc: 0.4310\n",
      "Epoch 61/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 1.9127 - acc: 0.4975 - val_loss: 2.1518 - val_acc: 0.4305\n",
      "Epoch 62/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 1.8917 - acc: 0.5030 - val_loss: 2.1456 - val_acc: 0.4300\n",
      "Epoch 63/70\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 1.8700 - acc: 0.5039 - val_loss: 2.1411 - val_acc: 0.4280\n",
      "Epoch 64/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 1.8675 - acc: 0.5046 - val_loss: 2.1365 - val_acc: 0.4365\n",
      "Epoch 65/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 1.8387 - acc: 0.5166 - val_loss: 2.1299 - val_acc: 0.4330\n",
      "Epoch 66/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 1.8332 - acc: 0.5119 - val_loss: 2.1229 - val_acc: 0.4375\n",
      "Epoch 67/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 1.8119 - acc: 0.5182 - val_loss: 2.1197 - val_acc: 0.4350\n",
      "Epoch 68/70\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 1.8036 - acc: 0.5201 - val_loss: 2.1135 - val_acc: 0.4360\n",
      "Epoch 69/70\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 1.7875 - acc: 0.5294 - val_loss: 2.1118 - val_acc: 0.4380\n",
      "Epoch 70/70\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 1.7542 - acc: 0.5272 - val_loss: 2.1057 - val_acc: 0.4360\n"
     ]
    }
   ],
   "source": [
    "#embed_dim = 50\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential() # modèle séquentiel\n",
    "model.add(tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 888,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True))# couche d'embedding de taille \n",
    "model.add(tf.keras.layers.Conv1D(256,\n",
    "                 2,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Compiler le modèle \n",
    "adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=adam ,metrics = ['accuracy'])\n",
    "# Fitter le modèle \n",
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 100, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 138us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0607304592132567, 0.448]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le modèle\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Modèle CNN multi-channel\n",
    "## Embedding non préentrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 4.2572 - acc: 0.0695 - val_loss: 3.8934 - val_acc: 0.1950\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 3.6274 - acc: 0.1891 - val_loss: 3.4757 - val_acc: 0.1880\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 3.4259 - acc: 0.1899 - val_loss: 3.4224 - val_acc: 0.1880\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 3.3694 - acc: 0.2009 - val_loss: 3.3794 - val_acc: 0.2015\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 3.3017 - acc: 0.2284 - val_loss: 3.3243 - val_acc: 0.2295\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 3.2279 - acc: 0.2487 - val_loss: 3.2544 - val_acc: 0.2480\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 3.1429 - acc: 0.2672 - val_loss: 3.1831 - val_acc: 0.2670\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 3.0605 - acc: 0.2846 - val_loss: 3.1190 - val_acc: 0.2780\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.9960 - acc: 0.2955 - val_loss: 3.0629 - val_acc: 0.2865\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.9242 - acc: 0.3100 - val_loss: 3.0122 - val_acc: 0.2950\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.8575 - acc: 0.3188 - val_loss: 2.9627 - val_acc: 0.3010\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.7966 - acc: 0.3244 - val_loss: 2.9139 - val_acc: 0.3075\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.7337 - acc: 0.3380 - val_loss: 2.8656 - val_acc: 0.3150\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.6702 - acc: 0.3515 - val_loss: 2.8191 - val_acc: 0.3240\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.6103 - acc: 0.3599 - val_loss: 2.7734 - val_acc: 0.3320\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.5469 - acc: 0.3750 - val_loss: 2.7296 - val_acc: 0.3390\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.4898 - acc: 0.3864 - val_loss: 2.6884 - val_acc: 0.3465\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.4379 - acc: 0.3997 - val_loss: 2.6501 - val_acc: 0.3500\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.3792 - acc: 0.4099 - val_loss: 2.6127 - val_acc: 0.3600\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.3248 - acc: 0.4215 - val_loss: 2.5779 - val_acc: 0.3645\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.2806 - acc: 0.4304 - val_loss: 2.5427 - val_acc: 0.3735\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.2287 - acc: 0.4457 - val_loss: 2.5117 - val_acc: 0.3750\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.1782 - acc: 0.4547 - val_loss: 2.4822 - val_acc: 0.3800\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.1340 - acc: 0.4669 - val_loss: 2.4537 - val_acc: 0.3900\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.0881 - acc: 0.4711 - val_loss: 2.4284 - val_acc: 0.3905\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 2.0410 - acc: 0.4865 - val_loss: 2.4046 - val_acc: 0.3970\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.9924 - acc: 0.4853 - val_loss: 2.3820 - val_acc: 0.4030\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.9522 - acc: 0.5058 - val_loss: 2.3639 - val_acc: 0.4010\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.9189 - acc: 0.5156 - val_loss: 2.3409 - val_acc: 0.4135\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.8709 - acc: 0.5231 - val_loss: 2.3223 - val_acc: 0.4160\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.8334 - acc: 0.5312 - val_loss: 2.3039 - val_acc: 0.4195\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.7979 - acc: 0.5420 - val_loss: 2.2864 - val_acc: 0.4190\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.7568 - acc: 0.5535 - val_loss: 2.2695 - val_acc: 0.4255\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.7146 - acc: 0.5655 - val_loss: 2.2532 - val_acc: 0.4270\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.6712 - acc: 0.5736 - val_loss: 2.2387 - val_acc: 0.4295\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.6408 - acc: 0.5782 - val_loss: 2.2237 - val_acc: 0.4310\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.5963 - acc: 0.5954 - val_loss: 2.2091 - val_acc: 0.4375\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.5584 - acc: 0.6082 - val_loss: 2.1962 - val_acc: 0.4390\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.5247 - acc: 0.6115 - val_loss: 2.1815 - val_acc: 0.4445\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.4801 - acc: 0.6299 - val_loss: 2.1701 - val_acc: 0.4470\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.4431 - acc: 0.6395 - val_loss: 2.1590 - val_acc: 0.4520\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.4029 - acc: 0.6530 - val_loss: 2.1454 - val_acc: 0.4540\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.3748 - acc: 0.6625 - val_loss: 2.1381 - val_acc: 0.4490\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.3305 - acc: 0.6674 - val_loss: 2.1254 - val_acc: 0.4565\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.2941 - acc: 0.6814 - val_loss: 2.1152 - val_acc: 0.4595\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.2580 - acc: 0.6941 - val_loss: 2.1058 - val_acc: 0.4610\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.2241 - acc: 0.7002 - val_loss: 2.0969 - val_acc: 0.4630\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.1904 - acc: 0.7129 - val_loss: 2.0869 - val_acc: 0.4625\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.1534 - acc: 0.7211 - val_loss: 2.0796 - val_acc: 0.4715\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.1166 - acc: 0.7365 - val_loss: 2.0723 - val_acc: 0.4710\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.0836 - acc: 0.7458 - val_loss: 2.0645 - val_acc: 0.4670\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.0536 - acc: 0.7496 - val_loss: 2.0603 - val_acc: 0.4695\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.0146 - acc: 0.7678 - val_loss: 2.0499 - val_acc: 0.4710\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9851 - acc: 0.7720 - val_loss: 2.0434 - val_acc: 0.4745\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9490 - acc: 0.7814 - val_loss: 2.0377 - val_acc: 0.4780\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9216 - acc: 0.7927 - val_loss: 2.0322 - val_acc: 0.4765\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8924 - acc: 0.8025 - val_loss: 2.0275 - val_acc: 0.4790\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8566 - acc: 0.8116 - val_loss: 2.0211 - val_acc: 0.4790\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8249 - acc: 0.8254 - val_loss: 2.0140 - val_acc: 0.4800\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.7995 - acc: 0.8296 - val_loss: 2.0135 - val_acc: 0.4775\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.7697 - acc: 0.8464 - val_loss: 2.0084 - val_acc: 0.4840\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.7491 - acc: 0.8484 - val_loss: 2.0037 - val_acc: 0.4800\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.7166 - acc: 0.8554 - val_loss: 1.9991 - val_acc: 0.4770\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6915 - acc: 0.8629 - val_loss: 1.9964 - val_acc: 0.4810\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6695 - acc: 0.8709 - val_loss: 1.9916 - val_acc: 0.4840\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6347 - acc: 0.8798 - val_loss: 1.9901 - val_acc: 0.4830\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.6084 - acc: 0.8896 - val_loss: 1.9860 - val_acc: 0.4855\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5891 - acc: 0.8960 - val_loss: 1.9841 - val_acc: 0.4840\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5633 - acc: 0.9004 - val_loss: 1.9827 - val_acc: 0.4850\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5408 - acc: 0.9079 - val_loss: 1.9798 - val_acc: 0.4870\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.5162 - acc: 0.9161 - val_loss: 1.9780 - val_acc: 0.4855\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4933 - acc: 0.9235 - val_loss: 1.9776 - val_acc: 0.4865\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4798 - acc: 0.9224 - val_loss: 1.9777 - val_acc: 0.4890\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4606 - acc: 0.9299 - val_loss: 1.9755 - val_acc: 0.4910\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4343 - acc: 0.9371 - val_loss: 1.9721 - val_acc: 0.4920\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4145 - acc: 0.9390 - val_loss: 1.9724 - val_acc: 0.4935\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3964 - acc: 0.9474 - val_loss: 1.9710 - val_acc: 0.4930\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3839 - acc: 0.9483 - val_loss: 1.9703 - val_acc: 0.4930\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3653 - acc: 0.9536 - val_loss: 1.9705 - val_acc: 0.4940\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3488 - acc: 0.9586 - val_loss: 1.9696 - val_acc: 0.4935\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3279 - acc: 0.9599 - val_loss: 1.9696 - val_acc: 0.4910\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3157 - acc: 0.9644 - val_loss: 1.9694 - val_acc: 0.4935\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.3084 - acc: 0.9655 - val_loss: 1.9697 - val_acc: 0.4965\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2879 - acc: 0.9708 - val_loss: 1.9695 - val_acc: 0.4945\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2748 - acc: 0.9719 - val_loss: 1.9713 - val_acc: 0.4950\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2630 - acc: 0.9759 - val_loss: 1.9711 - val_acc: 0.4945\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2501 - acc: 0.9788 - val_loss: 1.9718 - val_acc: 0.4970\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2437 - acc: 0.9789 - val_loss: 1.9712 - val_acc: 0.4980\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2288 - acc: 0.9816 - val_loss: 1.9713 - val_acc: 0.4965\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2105 - acc: 0.9851 - val_loss: 1.9707 - val_acc: 0.4965\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.2063 - acc: 0.9858 - val_loss: 1.9749 - val_acc: 0.4970\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1984 - acc: 0.9866 - val_loss: 1.9759 - val_acc: 0.5010\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1838 - acc: 0.9894 - val_loss: 1.9764 - val_acc: 0.4990\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1841 - acc: 0.9870 - val_loss: 1.9758 - val_acc: 0.4995\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1717 - acc: 0.9894 - val_loss: 1.9799 - val_acc: 0.4995\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1650 - acc: 0.9889 - val_loss: 1.9800 - val_acc: 0.4965\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1550 - acc: 0.9909 - val_loss: 1.9807 - val_acc: 0.4960\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1488 - acc: 0.9925 - val_loss: 1.9812 - val_acc: 0.4995\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1412 - acc: 0.9940 - val_loss: 1.9829 - val_acc: 0.5025\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.1333 - acc: 0.9939 - val_loss: 1.9863 - val_acc: 0.5020\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    filter_sizes = [2, 3, 5]\n",
    "    num_filters = 256\n",
    "    drop = 0.3\n",
    "    MAX_LENGTH=888\n",
    "    MAX_NB_WORDS = 10000\n",
    "    \n",
    "    inputs = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "    embedding = Embedding(input_dim=MAX_NB_WORDS,\n",
    "                                output_dim=embedding_dim,\n",
    "                                input_length=MAX_LENGTH)(inputs)\n",
    "\n",
    "    reshape = Reshape((MAX_LENGTH, embedding_dim, 1))(embedding)\n",
    "    conv_0 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[0], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    conv_1 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[1], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[2], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_0)\n",
    "\n",
    "    maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_1)\n",
    "\n",
    "    maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_2)\n",
    "    concatenated_tensor = Concatenate(axis=1)(\n",
    "        [maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    \n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    \n",
    "    #outpout1=Dense(units=256, activation='relu')(dropout)\n",
    "    #dropout1 = Dropout(drop)(outpout1)\n",
    "    \n",
    "    output = Dense(units=cat_vocab, activation='softmax')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn_model_multi_channel = get_cnn_model()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "history = cnn_model_multi_channel.fit(x=x_train_seq, \n",
    "                    y=y_train_id, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_test_seq, y_test_id),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 469us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9862929067611694, 0.502]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le modèle\n",
    "cnn_model_multi_channel.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Modèle CNN multi-channel\n",
    "## Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    embedding_dim = 50\n",
    "    filter_sizes = [2, 3, 5]\n",
    "    num_filters = 256\n",
    "    drop = 0.3\n",
    "    MAX_LENGTH=888\n",
    "    MAX_NB_WORDS = 10000\n",
    "    \n",
    "    inputs = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "    embedding = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 888,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True)(inputs)\n",
    "\n",
    "    reshape = Reshape((MAX_LENGTH, embedding_dim, 1))(embedding)\n",
    "    conv_0 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[0], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    conv_1 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[1], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, \n",
    "                    kernel_size=(filter_sizes[2], embedding_dim), \n",
    "                    padding='valid', kernel_initializer='normal', \n",
    "                    activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_0)\n",
    "\n",
    "    maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_1)\n",
    "\n",
    "    maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), \n",
    "                          strides=(1,1), padding='valid')(conv_2)\n",
    "    concatenated_tensor = Concatenate(axis=1)(\n",
    "        [maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    \n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    \n",
    "    #outpout1=Dense(units=256, activation='relu')(dropout)\n",
    "    #dropout1 = Dropout(drop)(outpout1)\n",
    "    \n",
    "    output = Dense(units=cat_vocab, activation='softmax')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_cnn_model()\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 8s 944us/step - loss: 4.2019 - acc: 0.1118 - val_loss: 3.4187 - val_acc: 0.1935\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 3.6717 - acc: 0.1575 - val_loss: 3.3163 - val_acc: 0.2275\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 3.5461 - acc: 0.1861 - val_loss: 3.2393 - val_acc: 0.2460\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 3.4448 - acc: 0.2075 - val_loss: 3.1699 - val_acc: 0.2750\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 3.3410 - acc: 0.2311 - val_loss: 3.1051 - val_acc: 0.2820\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 3.2622 - acc: 0.2454 - val_loss: 3.0483 - val_acc: 0.2875\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 3.1695 - acc: 0.2635 - val_loss: 2.9905 - val_acc: 0.3070\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 3.0959 - acc: 0.2775 - val_loss: 2.9402 - val_acc: 0.3125\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 3.0233 - acc: 0.2815 - val_loss: 2.8983 - val_acc: 0.3140\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 2.9636 - acc: 0.2931 - val_loss: 2.8504 - val_acc: 0.3280\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.9034 - acc: 0.3035 - val_loss: 2.8094 - val_acc: 0.3340\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 2.8370 - acc: 0.3177 - val_loss: 2.7698 - val_acc: 0.3340\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 2.7888 - acc: 0.3231 - val_loss: 2.7341 - val_acc: 0.3390\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.7369 - acc: 0.3369 - val_loss: 2.7015 - val_acc: 0.3365\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 6s 789us/step - loss: 2.6747 - acc: 0.3445 - val_loss: 2.6694 - val_acc: 0.3430\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 2.6303 - acc: 0.3535 - val_loss: 2.6379 - val_acc: 0.3490\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 2.5849 - acc: 0.3610 - val_loss: 2.6086 - val_acc: 0.3585\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.5332 - acc: 0.3716 - val_loss: 2.5802 - val_acc: 0.3570\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 2.4987 - acc: 0.3800 - val_loss: 2.5574 - val_acc: 0.3550\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.4522 - acc: 0.3874 - val_loss: 2.5339 - val_acc: 0.3645\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.4154 - acc: 0.3906 - val_loss: 2.5092 - val_acc: 0.3665\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.3746 - acc: 0.4024 - val_loss: 2.4830 - val_acc: 0.3710\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 6s 790us/step - loss: 2.3367 - acc: 0.4120 - val_loss: 2.4633 - val_acc: 0.3735\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 2.3160 - acc: 0.4161 - val_loss: 2.4414 - val_acc: 0.3755\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 2.2885 - acc: 0.4190 - val_loss: 2.4228 - val_acc: 0.3755\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.2515 - acc: 0.4254 - val_loss: 2.4052 - val_acc: 0.3760\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 2.2204 - acc: 0.4284 - val_loss: 2.3880 - val_acc: 0.3830\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.1834 - acc: 0.4366 - val_loss: 2.3711 - val_acc: 0.3830\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.1537 - acc: 0.4475 - val_loss: 2.3528 - val_acc: 0.3885\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.1278 - acc: 0.4483 - val_loss: 2.3376 - val_acc: 0.3845\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 2.0992 - acc: 0.4535 - val_loss: 2.3227 - val_acc: 0.3960\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 2.0675 - acc: 0.4599 - val_loss: 2.3063 - val_acc: 0.3910\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 6s 784us/step - loss: 2.0431 - acc: 0.4634 - val_loss: 2.2969 - val_acc: 0.3935\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 2.0209 - acc: 0.4734 - val_loss: 2.2803 - val_acc: 0.3970\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.9926 - acc: 0.4831 - val_loss: 2.2675 - val_acc: 0.4035\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.9658 - acc: 0.4880 - val_loss: 2.2548 - val_acc: 0.4040\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.9347 - acc: 0.4879 - val_loss: 2.2438 - val_acc: 0.4000\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.9097 - acc: 0.4956 - val_loss: 2.2340 - val_acc: 0.4070\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.8869 - acc: 0.5049 - val_loss: 2.2208 - val_acc: 0.4085\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 6s 789us/step - loss: 1.8670 - acc: 0.5088 - val_loss: 2.2090 - val_acc: 0.4115\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.8426 - acc: 0.5051 - val_loss: 2.1995 - val_acc: 0.4135\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 1.8226 - acc: 0.5134 - val_loss: 2.1913 - val_acc: 0.4115\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.8021 - acc: 0.5144 - val_loss: 2.1782 - val_acc: 0.4160\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.7740 - acc: 0.5281 - val_loss: 2.1698 - val_acc: 0.4165\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.7573 - acc: 0.5312 - val_loss: 2.1615 - val_acc: 0.4160\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.7346 - acc: 0.5374 - val_loss: 2.1531 - val_acc: 0.4225\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.7358 - acc: 0.5335 - val_loss: 2.1460 - val_acc: 0.4210\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.6971 - acc: 0.5484 - val_loss: 2.1383 - val_acc: 0.4160\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.6736 - acc: 0.5480 - val_loss: 2.1279 - val_acc: 0.4250\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.6531 - acc: 0.5555 - val_loss: 2.1198 - val_acc: 0.4275\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.6350 - acc: 0.5641 - val_loss: 2.1143 - val_acc: 0.4200\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.6154 - acc: 0.5657 - val_loss: 2.1054 - val_acc: 0.4255\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.5877 - acc: 0.5747 - val_loss: 2.1004 - val_acc: 0.4275\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 1.5787 - acc: 0.5764 - val_loss: 2.0908 - val_acc: 0.4285\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.5498 - acc: 0.5823 - val_loss: 2.0851 - val_acc: 0.4355\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.5366 - acc: 0.5890 - val_loss: 2.0798 - val_acc: 0.4335\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.5148 - acc: 0.5940 - val_loss: 2.0731 - val_acc: 0.4365\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.5108 - acc: 0.5920 - val_loss: 2.0667 - val_acc: 0.4400\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 6s 784us/step - loss: 1.4935 - acc: 0.6031 - val_loss: 2.0615 - val_acc: 0.4365\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 6s 785us/step - loss: 1.4669 - acc: 0.6034 - val_loss: 2.0548 - val_acc: 0.4370\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.4511 - acc: 0.6086 - val_loss: 2.0509 - val_acc: 0.4370\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 6s 789us/step - loss: 1.4324 - acc: 0.6169 - val_loss: 2.0433 - val_acc: 0.4405\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 6s 790us/step - loss: 1.4143 - acc: 0.6219 - val_loss: 2.0406 - val_acc: 0.4420\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 6s 790us/step - loss: 1.3803 - acc: 0.6286 - val_loss: 2.0342 - val_acc: 0.4450\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.3791 - acc: 0.6296 - val_loss: 2.0280 - val_acc: 0.4410\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.3559 - acc: 0.6411 - val_loss: 2.0236 - val_acc: 0.4465\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.3424 - acc: 0.6358 - val_loss: 2.0173 - val_acc: 0.4440\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.3148 - acc: 0.6462 - val_loss: 2.0143 - val_acc: 0.4455\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.3069 - acc: 0.6556 - val_loss: 2.0079 - val_acc: 0.4435\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.2894 - acc: 0.6525 - val_loss: 2.0077 - val_acc: 0.4485\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 1.2717 - acc: 0.6617 - val_loss: 2.0014 - val_acc: 0.4505\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.2529 - acc: 0.6619 - val_loss: 1.9968 - val_acc: 0.4505\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 6s 784us/step - loss: 1.2365 - acc: 0.6730 - val_loss: 1.9924 - val_acc: 0.4500\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.2292 - acc: 0.6707 - val_loss: 1.9872 - val_acc: 0.4525\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.2066 - acc: 0.6829 - val_loss: 1.9854 - val_acc: 0.4565\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.1806 - acc: 0.6851 - val_loss: 1.9839 - val_acc: 0.4525\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 1.1697 - acc: 0.6911 - val_loss: 1.9769 - val_acc: 0.4570\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 1.1556 - acc: 0.6964 - val_loss: 1.9752 - val_acc: 0.4545\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.1453 - acc: 0.6990 - val_loss: 1.9726 - val_acc: 0.4575\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 1.1265 - acc: 0.7005 - val_loss: 1.9681 - val_acc: 0.4615\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.1052 - acc: 0.7090 - val_loss: 1.9640 - val_acc: 0.4605\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 1.0906 - acc: 0.7166 - val_loss: 1.9618 - val_acc: 0.4575\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 1.0729 - acc: 0.7249 - val_loss: 1.9602 - val_acc: 0.4555\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.0566 - acc: 0.7238 - val_loss: 1.9555 - val_acc: 0.4550\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 1.0456 - acc: 0.7305 - val_loss: 1.9524 - val_acc: 0.4575\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.0318 - acc: 0.7334 - val_loss: 1.9505 - val_acc: 0.4635\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.0151 - acc: 0.7394 - val_loss: 1.9492 - val_acc: 0.4605\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 1.0033 - acc: 0.7478 - val_loss: 1.9464 - val_acc: 0.4630\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.9799 - acc: 0.7456 - val_loss: 1.9431 - val_acc: 0.4630\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.9737 - acc: 0.7516 - val_loss: 1.9405 - val_acc: 0.4605\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.9535 - acc: 0.7610 - val_loss: 1.9397 - val_acc: 0.4590\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.9377 - acc: 0.7602 - val_loss: 1.9353 - val_acc: 0.4660\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.9208 - acc: 0.7694 - val_loss: 1.9358 - val_acc: 0.4620\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 0.9153 - acc: 0.7701 - val_loss: 1.9366 - val_acc: 0.4585\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.8955 - acc: 0.7740 - val_loss: 1.9314 - val_acc: 0.4680\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.8804 - acc: 0.7835 - val_loss: 1.9289 - val_acc: 0.4660\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.8586 - acc: 0.7841 - val_loss: 1.9292 - val_acc: 0.4655\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.8466 - acc: 0.7965 - val_loss: 1.9265 - val_acc: 0.4650\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.8396 - acc: 0.7909 - val_loss: 1.9242 - val_acc: 0.4670\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.8304 - acc: 0.7925 - val_loss: 1.9232 - val_acc: 0.4695\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 100, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "8000/8000 [==============================] - 6s 789us/step - loss: 0.8192 - acc: 0.8050 - val_loss: 1.9205 - val_acc: 0.4715\n",
      "Epoch 2/20\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 0.8078 - acc: 0.8035 - val_loss: 1.9203 - val_acc: 0.4730\n",
      "Epoch 3/20\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 0.7850 - acc: 0.8126 - val_loss: 1.9235 - val_acc: 0.4660\n",
      "Epoch 4/20\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.7799 - acc: 0.8110 - val_loss: 1.9189 - val_acc: 0.4770\n",
      "Epoch 5/20\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.7606 - acc: 0.8199 - val_loss: 1.9157 - val_acc: 0.4735\n",
      "Epoch 6/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.7457 - acc: 0.8257 - val_loss: 1.9153 - val_acc: 0.4710\n",
      "Epoch 7/20\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.7504 - acc: 0.8191 - val_loss: 1.9144 - val_acc: 0.4745\n",
      "Epoch 8/20\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 0.7223 - acc: 0.8307 - val_loss: 1.9167 - val_acc: 0.4755\n",
      "Epoch 9/20\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.7132 - acc: 0.8327 - val_loss: 1.9139 - val_acc: 0.4755\n",
      "Epoch 10/20\n",
      "8000/8000 [==============================] - 6s 789us/step - loss: 0.7050 - acc: 0.8345 - val_loss: 1.9137 - val_acc: 0.4760\n",
      "Epoch 11/20\n",
      "8000/8000 [==============================] - 6s 789us/step - loss: 0.6826 - acc: 0.8424 - val_loss: 1.9121 - val_acc: 0.4755\n",
      "Epoch 12/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.6777 - acc: 0.8446 - val_loss: 1.9124 - val_acc: 0.4785\n",
      "Epoch 13/20\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.6738 - acc: 0.8464 - val_loss: 1.9112 - val_acc: 0.4785\n",
      "Epoch 14/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.6523 - acc: 0.8525 - val_loss: 1.9106 - val_acc: 0.4745\n",
      "Epoch 15/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.6418 - acc: 0.8582 - val_loss: 1.9080 - val_acc: 0.4745\n",
      "Epoch 16/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.6386 - acc: 0.8602 - val_loss: 1.9097 - val_acc: 0.4730\n",
      "Epoch 17/20\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.6221 - acc: 0.8615 - val_loss: 1.9102 - val_acc: 0.4760\n",
      "Epoch 18/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.6122 - acc: 0.8593 - val_loss: 1.9083 - val_acc: 0.4705\n",
      "Epoch 19/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.5976 - acc: 0.8681 - val_loss: 1.9098 - val_acc: 0.4740\n",
      "Epoch 20/20\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.5947 - acc: 0.8720 - val_loss: 1.9068 - val_acc: 0.4740\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 20, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 6s 793us/step - loss: 0.5809 - acc: 0.8705 - val_loss: 1.9087 - val_acc: 0.4760\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.5698 - acc: 0.8739 - val_loss: 1.9093 - val_acc: 0.4745\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.5576 - acc: 0.8795 - val_loss: 1.9091 - val_acc: 0.4765\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.5458 - acc: 0.8861 - val_loss: 1.9088 - val_acc: 0.4725\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.5383 - acc: 0.8872 - val_loss: 1.9088 - val_acc: 0.4810\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 6s 789us/step - loss: 0.5305 - acc: 0.8891 - val_loss: 1.9104 - val_acc: 0.4730\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.5106 - acc: 0.8971 - val_loss: 1.9072 - val_acc: 0.4800\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.5147 - acc: 0.8930 - val_loss: 1.9121 - val_acc: 0.4775\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.5053 - acc: 0.8955 - val_loss: 1.9085 - val_acc: 0.4740\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.4871 - acc: 0.9034 - val_loss: 1.9117 - val_acc: 0.4765\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.4751 - acc: 0.9052 - val_loss: 1.9086 - val_acc: 0.4780\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.4653 - acc: 0.9084 - val_loss: 1.9092 - val_acc: 0.4785\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.4635 - acc: 0.9094 - val_loss: 1.9102 - val_acc: 0.4790\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.4632 - acc: 0.9090 - val_loss: 1.9103 - val_acc: 0.4765\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.4510 - acc: 0.9116 - val_loss: 1.9096 - val_acc: 0.4765\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.4373 - acc: 0.9156 - val_loss: 1.9125 - val_acc: 0.4745\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.4255 - acc: 0.9203 - val_loss: 1.9109 - val_acc: 0.4775\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.4162 - acc: 0.9235 - val_loss: 1.9133 - val_acc: 0.4760\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.4103 - acc: 0.9234 - val_loss: 1.9121 - val_acc: 0.4770\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.4003 - acc: 0.9265 - val_loss: 1.9143 - val_acc: 0.4780\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3987 - acc: 0.9286 - val_loss: 1.9183 - val_acc: 0.4760\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3921 - acc: 0.9265 - val_loss: 1.9196 - val_acc: 0.4755\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3825 - acc: 0.9303 - val_loss: 1.9177 - val_acc: 0.4775\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3737 - acc: 0.9336 - val_loss: 1.9180 - val_acc: 0.4780\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 6s 783us/step - loss: 0.3661 - acc: 0.9349 - val_loss: 1.9203 - val_acc: 0.4745\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.3642 - acc: 0.9320 - val_loss: 1.9297 - val_acc: 0.4740\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3527 - acc: 0.9394 - val_loss: 1.9240 - val_acc: 0.4825\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3385 - acc: 0.9440 - val_loss: 1.9246 - val_acc: 0.4770\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.3445 - acc: 0.9407 - val_loss: 1.9247 - val_acc: 0.4770\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3368 - acc: 0.9434 - val_loss: 1.9280 - val_acc: 0.4825\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3270 - acc: 0.9457 - val_loss: 1.9267 - val_acc: 0.4765\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.3225 - acc: 0.9455 - val_loss: 1.9282 - val_acc: 0.4745\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.3180 - acc: 0.9465 - val_loss: 1.9298 - val_acc: 0.4745\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 0.3075 - acc: 0.9510 - val_loss: 1.9297 - val_acc: 0.4760\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 0.2996 - acc: 0.9526 - val_loss: 1.9343 - val_acc: 0.4785\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.3040 - acc: 0.9513 - val_loss: 1.9315 - val_acc: 0.4785\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.2911 - acc: 0.9513 - val_loss: 1.9407 - val_acc: 0.4820\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2868 - acc: 0.9530 - val_loss: 1.9328 - val_acc: 0.4760\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2798 - acc: 0.9566 - val_loss: 1.9405 - val_acc: 0.4765\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2739 - acc: 0.9584 - val_loss: 1.9371 - val_acc: 0.4780\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.2745 - acc: 0.9575 - val_loss: 1.9427 - val_acc: 0.4720\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2599 - acc: 0.9606 - val_loss: 1.9432 - val_acc: 0.4775\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.2555 - acc: 0.9637 - val_loss: 1.9458 - val_acc: 0.4805\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2543 - acc: 0.9626 - val_loss: 1.9450 - val_acc: 0.4785\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2528 - acc: 0.9617 - val_loss: 1.9482 - val_acc: 0.4790\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2446 - acc: 0.9647 - val_loss: 1.9496 - val_acc: 0.4815\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 6s 785us/step - loss: 0.2463 - acc: 0.9605 - val_loss: 1.9521 - val_acc: 0.4775\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2417 - acc: 0.9657 - val_loss: 1.9533 - val_acc: 0.4770\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2279 - acc: 0.9681 - val_loss: 1.9562 - val_acc: 0.4790\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.2247 - acc: 0.9675 - val_loss: 1.9550 - val_acc: 0.4790\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2148 - acc: 0.9708 - val_loss: 1.9584 - val_acc: 0.4715\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.2165 - acc: 0.9701 - val_loss: 1.9593 - val_acc: 0.4775\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.2183 - acc: 0.9673 - val_loss: 1.9587 - val_acc: 0.4810\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.2077 - acc: 0.9712 - val_loss: 1.9599 - val_acc: 0.4790\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1993 - acc: 0.9770 - val_loss: 1.9738 - val_acc: 0.4780\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.1995 - acc: 0.9735 - val_loss: 1.9638 - val_acc: 0.4755\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1956 - acc: 0.9742 - val_loss: 1.9661 - val_acc: 0.4755\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.1875 - acc: 0.9762 - val_loss: 1.9694 - val_acc: 0.4810\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1879 - acc: 0.9762 - val_loss: 1.9658 - val_acc: 0.4740\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1823 - acc: 0.9780 - val_loss: 1.9717 - val_acc: 0.4765\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1753 - acc: 0.9796 - val_loss: 1.9766 - val_acc: 0.4745\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1773 - acc: 0.9776 - val_loss: 1.9769 - val_acc: 0.4770\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1740 - acc: 0.9781 - val_loss: 1.9762 - val_acc: 0.4750\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.1694 - acc: 0.9786 - val_loss: 1.9807 - val_acc: 0.4730\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.1656 - acc: 0.9784 - val_loss: 1.9846 - val_acc: 0.4785\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.1544 - acc: 0.9841 - val_loss: 1.9841 - val_acc: 0.4765\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.1546 - acc: 0.9825 - val_loss: 1.9877 - val_acc: 0.4750\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.1576 - acc: 0.9813 - val_loss: 1.9901 - val_acc: 0.4775\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 6s 788us/step - loss: 0.1536 - acc: 0.9806 - val_loss: 1.9912 - val_acc: 0.4740\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 6s 786us/step - loss: 0.1567 - acc: 0.9809 - val_loss: 1.9901 - val_acc: 0.4745\n",
      "Epoch 71/100\n",
      "5120/8000 [==================>...........] - ETA: 2s - loss: 0.1513 - acc: 0.9822"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7261c9c8b6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 100, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluer le modèle\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Modèle basique: LSTM\n",
    "## Embedding non préentrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 888, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 888, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 93)                11997     \n",
      "=================================================================\n",
      "Total params: 1,588,061\n",
      "Trainable params: 1,588,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Créer un réseau à base de LSTM avec au minimum:\n",
    "# Embedding\n",
    "# Dropout\n",
    "# LSTM\n",
    "# Dropout\n",
    "# Classifieur\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(10000, embed_dim,input_length = 888))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_out)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab,activation='softmax'))\n",
    "\n",
    "# Compiler le modèle \n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "# Afficher le summary du modèle\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 107s 13ms/step - loss: 3.7580 - acc: 0.1775 - val_loss: 3.4724 - val_acc: 0.1880\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 3.3547 - acc: 0.2256 - val_loss: 3.1398 - val_acc: 0.2505\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 3.0287 - acc: 0.2587 - val_loss: 3.0237 - val_acc: 0.2555\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 2.8280 - acc: 0.2886 - val_loss: 3.0150 - val_acc: 0.2735\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 2.6424 - acc: 0.3314 - val_loss: 3.0171 - val_acc: 0.2735\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 2.4088 - acc: 0.3761 - val_loss: 3.0495 - val_acc: 0.2705\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 2.1742 - acc: 0.4375 - val_loss: 3.1854 - val_acc: 0.2890\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 1.9386 - acc: 0.4844 - val_loss: 3.2215 - val_acc: 0.2745\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 1.7549 - acc: 0.5254 - val_loss: 3.3504 - val_acc: 0.2690\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 1.5874 - acc: 0.5716 - val_loss: 3.5078 - val_acc: 0.2715\n"
     ]
    }
   ],
   "source": [
    "# Fitter le modèle \n",
    "\n",
    "history = model.fit(x_train_seq, y_train_id, batch_size = batch_size, epochs = 10, validation_data=(x_test_seq, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 31s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.5078332405090333, 0.2715]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le modèle\n",
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Modèle basique: LSTM\n",
    "## Embedding Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 888, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 888, 50)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 93)                11997     \n",
      "=================================================================\n",
      "Total params: 20,228,239\n",
      "Trainable params: 20,228,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lstm_out = 128\n",
    "batch_size = 128\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 888,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_out)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(cat_vocab,activation='softmax'))\n",
    "\n",
    "# Compiler le modèle \n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "# Afficher le summary du modèle\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 110s 14ms/step - loss: 3.7265 - acc: 0.1685 - val_loss: 3.4480 - val_acc: 0.1880\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 3.3810 - acc: 0.2121 - val_loss: 3.1523 - val_acc: 0.2470\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 3.1553 - acc: 0.2362 - val_loss: 3.0177 - val_acc: 0.2705\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 3.1533 - acc: 0.2477 - val_loss: 3.1173 - val_acc: 0.2380\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 3.1419 - acc: 0.2282 - val_loss: 3.0706 - val_acc: 0.2560\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 2.9738 - acc: 0.2570 - val_loss: 2.9205 - val_acc: 0.2645\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 2.9735 - acc: 0.2556 - val_loss: 3.0213 - val_acc: 0.2395\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 108s 14ms/step - loss: 2.9943 - acc: 0.2699 - val_loss: 2.8572 - val_acc: 0.2870\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 2.9118 - acc: 0.2749 - val_loss: 2.8647 - val_acc: 0.2645\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 2.7590 - acc: 0.2875 - val_loss: 2.7848 - val_acc: 0.2975\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = batch_size, epochs = 10, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 31s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.7848419647216796, 0.2975]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluer le modèle\n",
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Modèle basique: GRU\n",
    "## Embedding non préentrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn_model():\n",
    "    embedding_dim = 300\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=888\n",
    "    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    x = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, \n",
    "                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = Adam(lr=1e-3)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer= adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_simple_model = get_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 77s 10ms/step - loss: 3.5834 - acc: 0.1781 - val_loss: 3.4761 - val_acc: 0.1880\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 3.4467 - acc: 0.2003 - val_loss: 3.3628 - val_acc: 0.2370\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 3.2749 - acc: 0.2525 - val_loss: 3.1657 - val_acc: 0.2660\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 3.0723 - acc: 0.2812 - val_loss: 2.9977 - val_acc: 0.2810\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.8794 - acc: 0.3119 - val_loss: 2.8271 - val_acc: 0.3265\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.6928 - acc: 0.3405 - val_loss: 2.6730 - val_acc: 0.3445\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.5142 - acc: 0.3776 - val_loss: 2.5746 - val_acc: 0.3660\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.3674 - acc: 0.4012 - val_loss: 2.4869 - val_acc: 0.3940\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.2296 - acc: 0.4325 - val_loss: 2.4029 - val_acc: 0.3980\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.1054 - acc: 0.4572 - val_loss: 2.3553 - val_acc: 0.4025\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.0000 - acc: 0.4781 - val_loss: 2.3344 - val_acc: 0.4040\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.8837 - acc: 0.5071 - val_loss: 2.2771 - val_acc: 0.4200\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.7807 - acc: 0.5304 - val_loss: 2.2564 - val_acc: 0.4250\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.6759 - acc: 0.5524 - val_loss: 2.2387 - val_acc: 0.4325\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.5630 - acc: 0.5905 - val_loss: 2.2381 - val_acc: 0.4280\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.4824 - acc: 0.6036 - val_loss: 2.2384 - val_acc: 0.4260\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.3781 - acc: 0.6328 - val_loss: 2.2488 - val_acc: 0.4340\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.2855 - acc: 0.6525 - val_loss: 2.2648 - val_acc: 0.4305\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.2059 - acc: 0.6776 - val_loss: 2.2766 - val_acc: 0.4340\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.1277 - acc: 0.7002 - val_loss: 2.2908 - val_acc: 0.4260\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.0437 - acc: 0.7224 - val_loss: 2.3075 - val_acc: 0.4280\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.9732 - acc: 0.7464 - val_loss: 2.3186 - val_acc: 0.4245\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.8988 - acc: 0.7638 - val_loss: 2.3433 - val_acc: 0.4300\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.8357 - acc: 0.7814 - val_loss: 2.3671 - val_acc: 0.4210\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.7773 - acc: 0.8009 - val_loss: 2.3907 - val_acc: 0.4160\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.7182 - acc: 0.8225 - val_loss: 2.4305 - val_acc: 0.4165\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.6657 - acc: 0.8395 - val_loss: 2.4428 - val_acc: 0.4200\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.6176 - acc: 0.8536 - val_loss: 2.4645 - val_acc: 0.4190\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.5678 - acc: 0.8630 - val_loss: 2.5080 - val_acc: 0.4155\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.5268 - acc: 0.8765 - val_loss: 2.5269 - val_acc: 0.4115\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.4908 - acc: 0.8852 - val_loss: 2.5707 - val_acc: 0.4100\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.4469 - acc: 0.9011 - val_loss: 2.5969 - val_acc: 0.4160\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.4159 - acc: 0.9128 - val_loss: 2.6237 - val_acc: 0.4160\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.3756 - acc: 0.9239 - val_loss: 2.6640 - val_acc: 0.4080\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.3466 - acc: 0.9325 - val_loss: 2.6956 - val_acc: 0.4135\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.3239 - acc: 0.9381 - val_loss: 2.7216 - val_acc: 0.4065\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.2993 - acc: 0.9439 - val_loss: 2.7558 - val_acc: 0.4040\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.2739 - acc: 0.9531 - val_loss: 2.8043 - val_acc: 0.4010\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.2548 - acc: 0.9570 - val_loss: 2.8210 - val_acc: 0.3975\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.2334 - acc: 0.9640 - val_loss: 2.8419 - val_acc: 0.4050\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.2172 - acc: 0.9686 - val_loss: 2.8654 - val_acc: 0.4060\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.2008 - acc: 0.9700 - val_loss: 2.8907 - val_acc: 0.3985\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1891 - acc: 0.9753 - val_loss: 2.9122 - val_acc: 0.4020\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1705 - acc: 0.9776 - val_loss: 2.9395 - val_acc: 0.4055\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1620 - acc: 0.9802 - val_loss: 2.9661 - val_acc: 0.4080\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1487 - acc: 0.9820 - val_loss: 2.9971 - val_acc: 0.4020\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1365 - acc: 0.9861 - val_loss: 3.0247 - val_acc: 0.4100\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1285 - acc: 0.9862 - val_loss: 3.0375 - val_acc: 0.3975\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1190 - acc: 0.9910 - val_loss: 3.0492 - val_acc: 0.3985\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.1133 - acc: 0.9906 - val_loss: 3.0843 - val_acc: 0.4050\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "history = rnn_simple_model .fit(x=x_train_seq, \n",
    "                    y=y_train_id, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_test_seq, y_test_id),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 22s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.0843389625549316, 0.405]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_simple_model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Modèle basique: GRU\n",
    "## Embedding Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn_model():\n",
    "    embedding_dim = 50\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=888\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 888,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    adam = Adam(lr=1e-3)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer= adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 75s 9ms/step - loss: 3.5725 - acc: 0.1764 - val_loss: 3.3635 - val_acc: 0.1890\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 3.2621 - acc: 0.2371 - val_loss: 3.0685 - val_acc: 0.2480\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.9927 - acc: 0.2769 - val_loss: 2.8307 - val_acc: 0.2995\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.7826 - acc: 0.3120 - val_loss: 2.6298 - val_acc: 0.3295\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.6059 - acc: 0.3376 - val_loss: 2.5064 - val_acc: 0.3595\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 2.4680 - acc: 0.3634 - val_loss: 2.3704 - val_acc: 0.3830\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 2.3506 - acc: 0.3876 - val_loss: 2.2958 - val_acc: 0.3960\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 2.2368 - acc: 0.4140 - val_loss: 2.2659 - val_acc: 0.4020\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.1566 - acc: 0.4228 - val_loss: 2.1808 - val_acc: 0.4065\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.0742 - acc: 0.4365 - val_loss: 2.1329 - val_acc: 0.4195\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 1.9999 - acc: 0.4550 - val_loss: 2.1127 - val_acc: 0.4335\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 1.9348 - acc: 0.4675 - val_loss: 2.1015 - val_acc: 0.4190\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.8617 - acc: 0.4838 - val_loss: 2.0420 - val_acc: 0.4390\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.8030 - acc: 0.4984 - val_loss: 2.0156 - val_acc: 0.4410\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.7183 - acc: 0.5204 - val_loss: 2.0113 - val_acc: 0.4445\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 1.6694 - acc: 0.5299 - val_loss: 1.9926 - val_acc: 0.4370\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 1.6098 - acc: 0.5484 - val_loss: 2.0050 - val_acc: 0.4535\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.5525 - acc: 0.5571 - val_loss: 1.9849 - val_acc: 0.4610\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.4827 - acc: 0.5779 - val_loss: 2.0006 - val_acc: 0.4480\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.4442 - acc: 0.5820 - val_loss: 1.9824 - val_acc: 0.4585\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 1.3779 - acc: 0.6090 - val_loss: 1.9482 - val_acc: 0.4560\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 1.3247 - acc: 0.6218 - val_loss: 1.9838 - val_acc: 0.4510\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 1.2802 - acc: 0.6308 - val_loss: 1.9613 - val_acc: 0.4600\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.1935 - acc: 0.6609 - val_loss: 2.0181 - val_acc: 0.4520\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.1588 - acc: 0.6748 - val_loss: 2.0361 - val_acc: 0.4475\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.1116 - acc: 0.6841 - val_loss: 2.0204 - val_acc: 0.4555\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.0428 - acc: 0.7005 - val_loss: 2.0030 - val_acc: 0.4625\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.0142 - acc: 0.7137 - val_loss: 2.0251 - val_acc: 0.4690\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 0.9600 - acc: 0.7224 - val_loss: 2.0294 - val_acc: 0.4575\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 0.9123 - acc: 0.7418 - val_loss: 2.0653 - val_acc: 0.4685\n",
      "Epoch 31/50\n",
      "7296/8000 [==========================>...] - ETA: 5s - loss: 0.8500 - acc: 0.7596"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-10017d5b4e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_glove, y_train_id, batch_size = 128, epochs = 50, validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 20s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0637245044708252, 0.461]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_RNN\n",
    "\n",
    "## Embedding non préentrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8000/8000 [==============================] - 77s 10ms/step - loss: 3.6610 - acc: 0.1613 - val_loss: 3.5200 - val_acc: 0.1885\n",
      "Epoch 2/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 3.4997 - acc: 0.1903 - val_loss: 3.4395 - val_acc: 0.2140\n",
      "Epoch 3/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 3.3313 - acc: 0.2414 - val_loss: 3.1894 - val_acc: 0.2660\n",
      "Epoch 4/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 3.1172 - acc: 0.2742 - val_loss: 3.0598 - val_acc: 0.2795\n",
      "Epoch 5/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.9716 - acc: 0.2928 - val_loss: 2.9409 - val_acc: 0.2995\n",
      "Epoch 6/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.8312 - acc: 0.3170 - val_loss: 2.8344 - val_acc: 0.3130\n",
      "Epoch 7/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.6704 - acc: 0.3444 - val_loss: 2.7397 - val_acc: 0.3385\n",
      "Epoch 8/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.5260 - acc: 0.3728 - val_loss: 2.6293 - val_acc: 0.3500\n",
      "Epoch 9/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.3598 - acc: 0.4049 - val_loss: 2.5312 - val_acc: 0.3660\n",
      "Epoch 10/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.2272 - acc: 0.4303 - val_loss: 2.4932 - val_acc: 0.3760\n",
      "Epoch 11/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.0747 - acc: 0.4735 - val_loss: 2.4458 - val_acc: 0.3900\n",
      "Epoch 12/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.9080 - acc: 0.5054 - val_loss: 2.4670 - val_acc: 0.3805\n",
      "Epoch 13/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.7499 - acc: 0.5441 - val_loss: 2.4414 - val_acc: 0.3825\n",
      "Epoch 14/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.6176 - acc: 0.5714 - val_loss: 2.4488 - val_acc: 0.3890\n",
      "Epoch 15/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.4547 - acc: 0.6200 - val_loss: 2.4762 - val_acc: 0.3985\n"
     ]
    }
   ],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=888\n",
    "    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))\n",
    "    inp = Input(shape=(888, ))\n",
    "    x = Embedding(MAX_NB_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_rnn_cnn_model()\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "history = model.fit(x=x_train_seq, \n",
    "                    y=y_train_id, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_test_seq, y_test_id),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 22s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.476172409057617, 0.3985]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_seq, y_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_RNN\n",
    "\n",
    "## Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "8000/8000 [==============================] - 76s 10ms/step - loss: 3.5766 - acc: 0.1928 - val_loss: 3.3159 - val_acc: 0.2370\n",
      "Epoch 2/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 3.1665 - acc: 0.2623 - val_loss: 2.9289 - val_acc: 0.3060\n",
      "Epoch 3/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.8595 - acc: 0.3056 - val_loss: 2.6522 - val_acc: 0.3335\n",
      "Epoch 4/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.6522 - acc: 0.3357 - val_loss: 2.4993 - val_acc: 0.3665\n",
      "Epoch 5/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.4972 - acc: 0.3579 - val_loss: 2.3822 - val_acc: 0.3780\n",
      "Epoch 6/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.3592 - acc: 0.3867 - val_loss: 2.3050 - val_acc: 0.3810\n",
      "Epoch 7/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.2511 - acc: 0.4096 - val_loss: 2.2182 - val_acc: 0.3985\n",
      "Epoch 8/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 2.1611 - acc: 0.4174 - val_loss: 2.1920 - val_acc: 0.4105\n",
      "Epoch 9/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 2.0874 - acc: 0.4271 - val_loss: 2.1344 - val_acc: 0.4115\n",
      "Epoch 10/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.9925 - acc: 0.4591 - val_loss: 2.1106 - val_acc: 0.4200\n",
      "Epoch 11/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.9195 - acc: 0.4703 - val_loss: 2.0986 - val_acc: 0.4285\n",
      "Epoch 12/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.8494 - acc: 0.4883 - val_loss: 2.1069 - val_acc: 0.4240\n",
      "Epoch 13/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.7936 - acc: 0.5052 - val_loss: 2.1280 - val_acc: 0.4235\n",
      "Epoch 14/15\n",
      "8000/8000 [==============================] - 73s 9ms/step - loss: 1.7305 - acc: 0.5146 - val_loss: 2.0520 - val_acc: 0.4365\n",
      "Epoch 15/15\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 1.6503 - acc: 0.5378 - val_loss: 2.0943 - val_acc: 0.4300\n"
     ]
    }
   ],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 50\n",
    "    MAX_NB_WORDS = 10000\n",
    "    MAX_LENGTH=888\n",
    "    \n",
    "    inp = Input(shape=(888, ))\n",
    "    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],input_length = 888,\n",
    "                                    weights=[embedding_matrix], \n",
    "                              trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(cat_vocab, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_rnn_cnn_model()\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "history = model.fit(X_train_glove,\n",
    "                    y_train_id, \n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    validation_data=(X_test_glove, y_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 21s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.094347032546997, 0.43]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_glove, y_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
